{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\annaconda3\\envs\\nlp1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pandas import read_parquet\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"data/mBERT/fine\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_parquet(\"data/merge/train.parquet\")\n",
    "dev_data = read_parquet(\"data/merge/dev.parquet\")\n",
    "test_data = read_parquet(\"data/merge/test.parquet\")\n",
    "\n",
    "with open(\"data/merge/tags_2_idx.json\", \"r\") as f:\n",
    "    tags2idx = json.load(f)\n",
    "\n",
    "with open(\"data/merge/idx_2_tags.json\", \"r\") as f:\n",
    "    idx2tags = json.load(f)\n",
    "\n",
    "with open(\"data/merge/chars2idx.json\", \"r\") as f:\n",
    "    chars2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = train_data[\"tokens\"].values.tolist()\n",
    "tags_train = train_data[\"ner_tags\"].values.tolist()\n",
    "\n",
    "sentences_dev = dev_data[\"tokens\"].values.tolist()\n",
    "tags_dev = dev_data[\"ner_tags\"].values.tolist()\n",
    "\n",
    "sentences_test = test_data[\"tokens\"].values.tolist()\n",
    "tags_test = test_data[\"ner_tags\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LEN = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_label(tokenized_input, tags, tags_2_idx, idx_2_tags, label_all_tokens=True): \n",
    "    # tokenized_input refers to the sequences after tokenized\n",
    "    # tags refers to the original tags from dataset\n",
    "    # False:只为每个拆分token的第一个子词提供一个标签。\n",
    "    # True:在属于同一 token 的所有子词中提供相同的标签。\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []   \n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)                \n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(tags[word_idx])\n",
    "            except:\n",
    "                label_ids.append(-100) \n",
    "        else:\n",
    "            label_ids.append(tags[word_idx] if label_all_tokens else -100)\n",
    "        previous_word_idx = word_idx      \n",
    "    return label_ids\n",
    "\n",
    "def generate_tokenized_input(sentences_raw, tags_raw):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    for i in range(len(sentences_raw)):\n",
    "        tokenized_text = tokenizer(sentences_raw[i].tolist(), padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\", is_split_into_words=True)\n",
    "        extended_tags = align_label(tokenized_text, tags_raw[i], tags2idx, idx2tags)\n",
    "        sentences.append(tokenized_text)\n",
    "        tags.append(extended_tags)\n",
    "    return sentences, tags\n",
    "\n",
    "def generate_tokenized_input_with_words(sentences_raw, tags_raw):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    words = []\n",
    "    chars = []\n",
    "    for i in range(len(sentences_raw)):\n",
    "        tokenized_text = tokenizer(sentences_raw[i].tolist(), padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\", is_split_into_words=True)\n",
    "        extended_tags = align_label(tokenized_text, tags_raw[i], tags2idx, idx2tags)\n",
    "        sentences.append(tokenized_text)\n",
    "        tags.append(extended_tags)\n",
    "        token_ids = tokenized_text[\"input_ids\"][0]\n",
    "        token_words = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        words.append(token_words)\n",
    "        char_ids = torch.zeros(512, MAX_WORD_LEN)\n",
    "        for i in range(len(token_words)):\n",
    "            for j in range(len(token_words[i])):\n",
    "                char_ids[i][j] = chars2idx.get(token_words[i][j], chars2idx['<unk>'])\n",
    "        chars.append(char_ids)\n",
    "    return sentences, tags, words, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_tags = generate_tokenized_input(sentences_train, tags_train)\n",
    "dev_sentences, dev_tags = generate_tokenized_input(sentences_dev, tags_dev)\n",
    "test_sentences, test_tags = generate_tokenized_input(sentences_test, tags_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_ids = train_sentences[0][\"input_ids\"][0]\n",
    "# token_words = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "# print(token_words)\n",
    "# char_ids = torch.zeros(512, MAX_WORD_LEN)\n",
    "# for i in range(len(token_words)):\n",
    "#     for j in range(len(token_words[i])):\n",
    "#         char_ids[i][j] = chars2idx.get(token_words[i][j], chars2idx['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences1, train_tags1, train_words, train_chars = generate_tokenized_input_with_words(sentences_train, tags_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\annaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained(\"data/mBert/fine\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_text_tokenized(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "    def get_labels(self, idx):\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_tokenized = self.get_text_tokenized(idx)\n",
    "        labels = self.get_labels(idx)\n",
    "        return text_tokenized, labels.unsqueeze(0)  # shap: [1, 512]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    text_tokenized_seqs, labels_seqs = zip(*batch)\n",
    "    B = len(labels_seqs)\n",
    "    batch_input_ids = []\n",
    "    batch_attention_masks = []\n",
    "    batch_label_seqs = torch.concat(labels_seqs)\n",
    "    for i in range(B):\n",
    "        batch_input_ids.append(text_tokenized_seqs[i][\"input_ids\"])\n",
    "        batch_attention_masks.append(text_tokenized_seqs[i][\"attention_mask\"])\n",
    "    batch_input_ids = torch.concat(batch_input_ids)\n",
    "    batch_attention_masks = torch.concat(batch_attention_masks)\n",
    "    bert_output = bert(batch_input_ids, batch_attention_masks)\n",
    "    bert_embeddings = bert_output[\"last_hidden_state\"]\n",
    "    return bert_embeddings, batch_label_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultilingualDataset(train_sentences, train_tags)\n",
    "dev_dataset = MultilingualDataset(dev_sentences, dev_tags)\n",
    "test_dataset = MultilingualDataset(test_sentences, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert(train_sentences[0][\"input_ids\"], attention_mask=train_sentences[0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 768]) torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "for data in train_loader:\n",
    "    bert_embeddings, labels = data\n",
    "    print(bert_embeddings.shape, labels.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
