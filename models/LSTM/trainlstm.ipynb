{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory (GB): 8.00\n",
      "Current Memory Allocated (GB): 0.00\n",
      "Current Memory Reserved (GB): 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you want to check memory for the first GPU\n",
    "gpu_index = 0\n",
    "\n",
    "# Get total GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "# Convert bytes to GB for easier interpretation\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_allocated_gb = current_memory_allocated / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory reserved by PyTorch's memory allocator\n",
    "current_memory_reserved = torch.cuda.memory_reserved(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_reserved_gb = current_memory_reserved / (1024 ** 3)\n",
    "\n",
    "print(f\"Total GPU Memory (GB): {total_memory_gb:.2f}\")\n",
    "print(f\"Current Memory Allocated (GB): {current_memory_allocated_gb:.2f}\")\n",
    "print(f\"Current Memory Reserved (GB): {current_memory_reserved_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from pandas import read_parquet\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model.eval()\n",
    "bert_model.to('cuda:0')\n",
    "def pooling_embedding(tokenized_input, embeddings):\n",
    "    processed_embedding = []\n",
    "    current_embedding = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for i, word_idx in enumerate(tokenized_input):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "\n",
    "        if word_idx == previous_word_idx:\n",
    "            current_embedding.append(embeddings[i])\n",
    "        else:\n",
    "            if current_embedding:\n",
    "                processed_embedding.append(torch.mean(torch.stack(current_embedding), dim=0))\n",
    "                current_embedding.clear()\n",
    "            current_embedding.append(embeddings[i])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "    if current_embedding:\n",
    "        processed_embedding.append(torch.mean(torch.stack(current_embedding), dim=0))\n",
    "\n",
    "    return torch.stack(processed_embedding)\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_file_path,tokenizer,bert_model):\n",
    "        self.raw_dataset=read_parquet(data_file_path)\n",
    "        self.tokenizer=tokenizer\n",
    "        self.bert_model=bert_model\n",
    "    def __len__(self):\n",
    "        return self.raw_dataset['tokens'].size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence_words = self.raw_dataset['tokens'][index].tolist()\n",
    "        encoded_words= tokenizer(sentence_words, return_tensors='pt',is_split_into_words=True,truncation=True).to(\"cuda:0\")\n",
    "        embeddings=self.bert_model(**encoded_words)\n",
    "        pooled_embeddings=pooling_embedding(encoded_words.word_ids(),embeddings.last_hidden_state[0])\n",
    "        labels = torch.tensor(self.raw_dataset['ner_tags'][index].astype(int)).to(\"cuda:0\")\n",
    "        if pooled_embeddings.shape[0]<labels.shape[0]:\n",
    "            labels=labels[:pooled_embeddings.shape[0]]\n",
    "        assert pooled_embeddings.shape[0]==labels.shape[0], f\"pooled_embeddings shape {pooled_embeddings.shape} and labels shape {labels.shape} are not equal, index {index}\"\n",
    "        return pooled_embeddings,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "first_param_device = next(bert_model.parameters()).device\n",
    "print(first_param_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataSetFromFile(Dataset):\n",
    "    def __init__(self, data_file_path):\n",
    "        self.data=torch.load(data_file_path)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, label_ids = zip(*batch)  # Unpack original sentences too\n",
    "    input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)\n",
    "    label_ids = pad_sequence([torch.tensor(ids) for ids in label_ids], batch_first=True, padding_value=-100)\n",
    "    return input_ids, label_ids  # Return original sentences as well\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from colleval import evaluate\n",
    "class BLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_labels, dropout):\n",
    "        super(BLSTMModel, self).__init__()\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        blstm_out, _ = self.blstm(input_ids)\n",
    "        blstm_out = self.dropout(blstm_out)\n",
    "        linear_out = self.linear(blstm_out)\n",
    "        elu_out = self.elu(linear_out)\n",
    "        logits = self.classifier(elu_out)\n",
    "        return logits\n",
    "# Hyperparameters\n",
    "embedding_dim = bert_model.config.hidden_size\n",
    "hidden_dim = 256\n",
    "output_dim = 128\n",
    "dropout = 0.33\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 60\n",
    "num_labels=9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = BLSTMModel(embedding_dim, hidden_dim, output_dim, num_labels, dropout)\n",
    "# model.load_state_dict(torch.load('best_model7514.pt'))\n",
    "lstm_model.to(\"cuda:0\")\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "scheduler=optim.lr_scheduler.LinearLR(optimizer,start_factor=0.9,end_factor=0.1,total_iters=60)\n",
    "#optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "# Learning rate scheduling\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataset=NERDataset(\"../../data/english/train-00000-of-00001.parquet\",tokenizer,bert_model)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = NERDataset(\"../../data/english/validation-00000-of-00001.parquet\",tokenizer,bert_model)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_f1_score=0\n",
    "patience = 15  # Number of epochs to wait for improvement\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(\"cuda:0\")\n",
    "        labels = labels.to(\"cuda:0\")\n",
    "        optimizer.zero_grad()\n",
    "        logits = lstm_model(input_ids)\n",
    "        loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation step\n",
    "    lstm_model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(\"cuda:0\")\n",
    "            labels = labels.to(\"cuda:0\")\n",
    "            logits = lstm_model(input_ids)\n",
    "            # predictions = torch.argmax(logits, dim=-1)\n",
    "            # for i, sentence in enumerate(input_ids):\n",
    "            #     for j, word_id in enumerate(sentence):\n",
    "            #         if word_id.item() == vocab['<PAD>']:\n",
    "            #             continue\n",
    "            #         # Get the predicted label name\n",
    "            #         predicted_label = reverse_label_map[predictions[i][j].item()]# Map the label ID back to its string representation\n",
    "            #         true_label = reverse_label_map[labels[i][j].item()]  # Map the label ID back to its string representation\n",
    "            #         true_labels.append(true_label)\n",
    "            #         pred_labels.append(predicted_label)\n",
    "            # loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # precision,recall,f1=evaluate(true_labels,pred_labels,False)\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved from {best_val_loss:.4f}--->{val_loss:.4f}\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(lstm_model.state_dict(), 'best_model3.pt')\n",
    "        epochs_without_improvement = 0\n",
    "    # if best_f1_score < f1:\n",
    "    #     print(f\"Validation f1 improved from {best_f1_score:.4f}--->{f1:.4f}\")\n",
    "    #     best_f1_score = f1\n",
    "    #     torch.save(lstm_model.state_dict(), 'best_model3.pt')\n",
    "    #     epochs_without_improvement = 0\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "    #     if epochs_without_improvement >= patience:\n",
    "    #         print(f'Early stopping at epoch {epoch + 1}')\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpoint-v1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpoint-v1.ckpt\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b0baf92c30436fbb56edfd82ad16d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55735/2979999473.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)\n",
      "/tmp/ipykernel_55735/2979999473.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_ids = pad_sequence([torch.tensor(ids) for ids in label_ids], batch_first=True, padding_value=-100)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from colleval import evaluate\n",
    "from datasets import load_metric\n",
    "wikineural_tags_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "wikineural_tags_to_int = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "wikineural_int_to_tags = {v: k for k,v in wikineural_tags_to_int.items()}\n",
    "\n",
    "\n",
    "train_dataset=NERDataset(\"../../data/english/train-00000-of-00001.parquet\",tokenizer,bert_model)\n",
    "\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = NERDataset(\"../../data/english/validation-00000-of-00001.parquet\",tokenizer,bert_model)\n",
    "test_dataset= NERDataset(\"../../data/english/test-00000-of-00001.parquet\",tokenizer,bert_model)\n",
    "# Create a TensorBoard logger\n",
    "logger = TensorBoardLogger(\"logs/\", name=\"my_model_lstm\")\n",
    "seqeval_metric = load_metric(\"seqeval\")\n",
    "class BLSTMModelLightning(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_labels, dropout):\n",
    "        super(BLSTMModelLightning, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        blstm_out, _ = self.blstm(input_ids)\n",
    "        blstm_out = self.dropout(blstm_out)\n",
    "        linear_out = self.linear(blstm_out)\n",
    "        elu_out = self.elu(linear_out)\n",
    "        logits = self.classifier(elu_out)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        mask = labels.view(-1) != -100\n",
    "        \n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        acc = accuracy(preds, labels_flat, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        # Create a mask to exclude padding tokens\n",
    "        mask = labels.view(-1) != -100\n",
    "        \n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        acc = accuracy(preds, labels_flat, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        # Create a mask to exclude padding tokens\n",
    "        mask = labels.view(-1) != -100\n",
    "        \n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        acc = accuracy(preds, labels_flat, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i,label in enumerate(labels_flat):\n",
    "            true_labels.append(wikineural_int_to_tags[label.item()])\n",
    "            pred_labels.append(wikineural_int_to_tags[preds[i].item()])\n",
    "        results = seqeval_metric.compute(predictions=[pred_labels], references=[true_labels])\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_seqeval\", results['overall_f1'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "class NERDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, test_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset=test_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "lstm_model = BLSTMModelLightning(embedding_dim, hidden_dim, output_dim, num_labels, dropout)\n",
    "data_module = NERDataModule(train_dataset, val_dataset, test_dataset,batch_size)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# trainer.fit(lstm_model, datamodule=data_module,ckpt_path=\"/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpoint-v1.ckpt\")\n",
    "trainer.test(lstm_model,datamodule=data_module,ckpt_path=\"/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpoint-v1.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
