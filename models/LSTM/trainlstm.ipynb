{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory (GB): 8.00\n",
      "Current Memory Allocated (GB): 0.00\n",
      "Current Memory Reserved (GB): 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you want to check memory for the first GPU\n",
    "gpu_index = 0\n",
    "\n",
    "# Get total GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "# Convert bytes to GB for easier interpretation\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_allocated_gb = current_memory_allocated / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory reserved by PyTorch's memory allocator\n",
    "current_memory_reserved = torch.cuda.memory_reserved(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_reserved_gb = current_memory_reserved / (1024 ** 3)\n",
    "\n",
    "print(f\"Total GPU Memory (GB): {total_memory_gb:.2f}\")\n",
    "print(f\"Current Memory Allocated (GB): {current_memory_allocated_gb:.2f}\")\n",
    "print(f\"Current Memory Reserved (GB): {current_memory_reserved_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from pandas import read_parquet\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model.eval()\n",
    "bert_model.to('cuda:0')\n",
    "\n",
    "\n",
    "def pooling_embedding(tokenized_input, embeddings):\n",
    "    processed_embedding = []\n",
    "    current_embedding = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for i, word_idx in enumerate(tokenized_input):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "\n",
    "        if word_idx == previous_word_idx:\n",
    "            current_embedding.append(embeddings[i])\n",
    "        else:\n",
    "            if current_embedding:\n",
    "                processed_embedding.append(torch.mean(\n",
    "                    torch.stack(current_embedding), dim=0))\n",
    "                current_embedding.clear()\n",
    "            current_embedding.append(embeddings[i])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "    if current_embedding:\n",
    "        processed_embedding.append(torch.mean(\n",
    "            torch.stack(current_embedding), dim=0))\n",
    "\n",
    "    return torch.stack(processed_embedding)\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_file_path, tokenizer, bert_model):\n",
    "        self.raw_dataset = read_parquet(data_file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.raw_dataset['tokens'].size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        current_row=self.raw_dataset.iloc[index]\n",
    "        sentence_words =current_row['tokens'].tolist()\n",
    "        encoded_words = tokenizer(sentence_words, return_tensors='pt',\n",
    "                                  is_split_into_words=True, truncation=True).to(\"cuda:0\")\n",
    "        embeddings = self.bert_model(**encoded_words)\n",
    "        pooled_embeddings = pooling_embedding(\n",
    "            encoded_words.word_ids(), embeddings.last_hidden_state[0])\n",
    "        labels = torch.tensor(\n",
    "            current_row['ner_tags'].astype(int)).to(\"cuda:0\")\n",
    "        if pooled_embeddings.shape[0] < labels.shape[0]:\n",
    "            labels = labels[:pooled_embeddings.shape[0]]\n",
    "        assert pooled_embeddings.shape[0] == labels.shape[\n",
    "            0], f\"pooled_embeddings shape {pooled_embeddings.shape} and labels shape {labels.shape} are not equal, index {index}\"\n",
    "        return pooled_embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "first_param_device = next(bert_model.parameters()).device\n",
    "print(first_param_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, label_ids = zip(*batch)  # Unpack original sentences too\n",
    "    input_ids = pad_sequence(\n",
    "        [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)\n",
    "    label_ids = pad_sequence(\n",
    "        [torch.tensor(ids) for ids in label_ids], batch_first=True, padding_value=-100)\n",
    "    return input_ids, label_ids  # Return original sentences as well\n",
    "\n",
    "\n",
    "# from colleval import evaluate\n",
    "\n",
    "\n",
    "class BLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_labels, dropout):\n",
    "        super(BLSTMModel, self).__init__()\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                             num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        blstm_out, _ = self.blstm(input_ids)\n",
    "        blstm_out = self.dropout(blstm_out)\n",
    "        linear_out = self.linear(blstm_out)\n",
    "        elu_out = self.elu(linear_out)\n",
    "        logits = self.classifier(elu_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = bert_model.config.hidden_size\n",
    "hidden_dim = 256\n",
    "output_dim = 128\n",
    "dropout = 0.33\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 60\n",
    "num_labels = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m read_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/merge/train.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m NERDataset(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/merge/train.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, bert_model)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m, in \u001b[0;36mNERDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     44\u001b[0m     sentence_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 45\u001b[0m     encoded_words\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_words)\n\u001b[1;32m     47\u001b[0m     pooled_embeddings\u001b[38;5;241m=\u001b[39mpooling_embedding(encoded_words\u001b[38;5;241m.\u001b[39mword_ids(),embeddings\u001b[38;5;241m.\u001b[39mlast_hidden_state[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2872\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2930\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2931\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2932\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2933\u001b[0m     )\n\u001b[1;32m   2935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2936\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2939\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "read_parquet(\"../../data/merge/train.parquet\")\n",
    "train_dataset = NERDataset(\n",
    "    \"../../data/merge/train.parquet\", tokenizer, bert_model)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = BLSTMModel(embedding_dim, hidden_dim,\n",
    "                        output_dim, num_labels, dropout)\n",
    "# model.load_state_dict(torch.load('best_model7514.pt'))\n",
    "lstm_model.to(\"cuda:0\")\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=0.9, end_factor=0.1, total_iters=60)\n",
    "# optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "# Learning rate scheduling\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataset = NERDataset(\n",
    "    \"../../data/english/train-00000-of-00001.parquet\", tokenizer, bert_model)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = NERDataset(\n",
    "    \"../../data/english/validation-00000-of-00001.parquet\", tokenizer, bert_model)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_f1_score = 0\n",
    "patience = 15  # Number of epochs to wait for improvement\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(\"cuda:0\")\n",
    "        labels = labels.to(\"cuda:0\")\n",
    "        optimizer.zero_grad()\n",
    "        logits = lstm_model(input_ids)\n",
    "        loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation step\n",
    "    lstm_model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(\"cuda:0\")\n",
    "            labels = labels.to(\"cuda:0\")\n",
    "            logits = lstm_model(input_ids)\n",
    "            # predictions = torch.argmax(logits, dim=-1)\n",
    "            # for i, sentence in enumerate(input_ids):\n",
    "            #     for j, word_id in enumerate(sentence):\n",
    "            #         if word_id.item() == vocab['<PAD>']:\n",
    "            #             continue\n",
    "            #         # Get the predicted label name\n",
    "            #         predicted_label = reverse_label_map[predictions[i][j].item()]# Map the label ID back to its string representation\n",
    "            #         true_label = reverse_label_map[labels[i][j].item()]  # Map the label ID back to its string representation\n",
    "            #         true_labels.append(true_label)\n",
    "            #         pred_labels.append(predicted_label)\n",
    "            # loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # precision,recall,f1=evaluate(true_labels,pred_labels,False)\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\n",
    "            f\"Validation loss improved from {best_val_loss:.4f}--->{val_loss:.4f}\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(lstm_model.state_dict(), 'best_model3.pt')\n",
    "        epochs_without_improvement = 0\n",
    "    # if best_f1_score < f1:\n",
    "    #     print(f\"Validation f1 improved from {best_f1_score:.4f}--->{f1:.4f}\")\n",
    "    #     best_f1_score = f1\n",
    "    #     torch.save(lstm_model.state_dict(), 'best_model3.pt')\n",
    "    #     epochs_without_improvement = 0\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "    #     if epochs_without_improvement >= patience:\n",
    "    #         print(f'Early stopping at epoch {epoch + 1}')\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | criterion  | CrossEntropyLoss | 0     \n",
      "1 | blstm      | LSTM             | 2.1 M \n",
      "2 | dropout    | Dropout          | 0     \n",
      "3 | linear     | Linear           | 65.7 K\n",
      "4 | elu        | ELU              | 0     \n",
      "5 | classifier | Linear           | 1.2 K \n",
      "------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.672     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d5ab452a2c426fbd403f2f1d9c41c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/tmp/ipykernel_55735/2979999473.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)\n",
      "/tmp/ipykernel_55735/2979999473.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_ids = pad_sequence([torch.tensor(ids) for ids in label_ids], batch_first=True, padding_value=-100)\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5a631adf9546efaac4c7bf6365eeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from colleval import evaluate\n",
    "from datasets import load_metric\n",
    "wikineural_tags_list = ['O', 'B-PER', 'I-PER', 'B-ORG',\n",
    "                        'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "wikineural_tags_to_int = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3,\n",
    "                          'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "wikineural_int_to_tags = {v: k for k, v in wikineural_tags_to_int.items()}\n",
    "\n",
    "\n",
    "train_dataset = NERDataset(\n",
    "    \"../../data/merge/train.parquet\", tokenizer, bert_model)\n",
    "\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = NERDataset(\n",
    "    \"../../data/merge/dev.parquet\", tokenizer, bert_model)\n",
    "test_dataset = NERDataset(\n",
    "    \"../../data/merge/test.parquet\", tokenizer, bert_model)\n",
    "# Create a TensorBoard logger\n",
    "logger = TensorBoardLogger(\"logs/\", name=\"my_model_lstm\")\n",
    "seqeval_metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "class BLSTMModelLightning(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_labels, dropout):\n",
    "        super(BLSTMModelLightning, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                             num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        blstm_out, _ = self.blstm(input_ids)\n",
    "        blstm_out = self.dropout(blstm_out)\n",
    "        linear_out = self.linear(blstm_out)\n",
    "        elu_out = self.elu(linear_out)\n",
    "        logits = self.classifier(elu_out)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(\n",
    "            logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        mask = labels.view(-1) != -100\n",
    "\n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        # acc = accuracy(preds, labels_flat, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        self.log(\"train_loss\", loss, on_step=False,\n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(\n",
    "            logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        # Create a mask to exclude padding tokens\n",
    "        mask = labels.view(-1) != -100\n",
    "\n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        # acc = accuracy(preds, labels_flat, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i, label in enumerate(labels_flat):\n",
    "            true_labels.append(wikineural_int_to_tags[label.item()])\n",
    "            pred_labels.append(wikineural_int_to_tags[preds[i].item()])\n",
    "        results = seqeval_metric.compute(\n",
    "            predictions=[pred_labels], references=[true_labels])\n",
    "        self.log(\"val_seqeval_f1\", results['overall_f1'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(\n",
    "            logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        # Create a mask to exclude padding tokens\n",
    "        mask = labels.view(-1) != -100\n",
    "\n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        acc = accuracy(preds, labels_flat, task=\"multiclass\",\n",
    "                       num_classes=self.num_labels)\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i, label in enumerate(labels_flat):\n",
    "            true_labels.append(wikineural_int_to_tags[label.item()])\n",
    "            pred_labels.append(wikineural_int_to_tags[preds[i].item()])\n",
    "        results = seqeval_metric.compute(\n",
    "            predictions=[pred_labels], references=[true_labels])\n",
    "        self.log(\"test_loss\", loss, on_step=False,\n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_seqeval_f1\", results['overall_f1'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "class NERDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, test_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpointt\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_seqeval_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "lstm_model = BLSTMModelLightning(\n",
    "    embedding_dim, hidden_dim, output_dim, num_labels, dropout)\n",
    "data_module = NERDataModule(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "trainer.fit(lstm_model, datamodule=data_module)\n",
    "# trainer.test(lstm_model,datamodule=data_module,ckpt_path=\"/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpoint-v1.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
