{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory (GB): 8.00\n",
      "Current Memory Allocated (GB): 0.00\n",
      "Current Memory Reserved (GB): 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you want to check memory for the first GPU\n",
    "gpu_index = 0\n",
    "\n",
    "# Get total GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "# Convert bytes to GB for easier interpretation\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_allocated_gb = current_memory_allocated / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory reserved by PyTorch's memory allocator\n",
    "current_memory_reserved = torch.cuda.memory_reserved(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_reserved_gb = current_memory_reserved / (1024 ** 3)\n",
    "\n",
    "print(f\"Total GPU Memory (GB): {total_memory_gb:.2f}\")\n",
    "print(f\"Current Memory Allocated (GB): {current_memory_allocated_gb:.2f}\")\n",
    "print(f\"Current Memory Reserved (GB): {current_memory_reserved_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../../data/mBERT/fine and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from pandas import read_parquet\n",
    "from transformers import BertTokenizer, BertModel,BertTokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "model_path = \"../../data/mBERT/fine\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "bert_model = BertModel.from_pretrained(model_path)\n",
    "bert_model.eval()\n",
    "bert_model.to('cuda:0')\n",
    "\n",
    "\n",
    "def pooling_embedding(tokenized_input, embeddings):\n",
    "    processed_embedding = []\n",
    "    current_embedding = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for i, word_idx in enumerate(tokenized_input):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "\n",
    "        if word_idx == previous_word_idx:\n",
    "            current_embedding.append(embeddings[i])\n",
    "        else:\n",
    "            if current_embedding:\n",
    "                processed_embedding.append(torch.mean(\n",
    "                    torch.stack(current_embedding), dim=0))\n",
    "                current_embedding.clear()\n",
    "            current_embedding.append(embeddings[i])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "    if current_embedding:\n",
    "        processed_embedding.append(torch.mean(\n",
    "            torch.stack(current_embedding), dim=0))\n",
    "\n",
    "    return torch.stack(processed_embedding)\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_file_path, tokenizer, bert_model):\n",
    "        self.raw_dataset = read_parquet(data_file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.raw_dataset['tokens'].size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        current_row=self.raw_dataset.iloc[index]\n",
    "        sentence_words =current_row['tokens'].tolist()\n",
    "        encoded_words = tokenizer(sentence_words, return_tensors='pt',\n",
    "                                  is_split_into_words=True, truncation=True).to(\"cuda:0\")\n",
    "        embeddings = self.bert_model(**encoded_words)\n",
    "        pooled_embeddings = pooling_embedding(\n",
    "            encoded_words.word_ids(), embeddings.last_hidden_state[0])\n",
    "        labels = torch.tensor(\n",
    "            current_row['ner_tags'].astype(int)).to(\"cuda:0\")\n",
    "        if pooled_embeddings.shape[0] < labels.shape[0]:\n",
    "            labels = labels[:pooled_embeddings.shape[0]]\n",
    "        assert pooled_embeddings.shape[0] == labels.shape[\n",
    "            0], f\"pooled_embeddings shape {pooled_embeddings.shape} and labels shape {labels.shape} are not equal, index {index}\"\n",
    "        return pooled_embeddings, labels\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, label_ids = zip(*batch)  # Unpack original sentences too\n",
    "    input_ids = pad_sequence([ids.clone().detach() for ids in input_ids], batch_first=True, padding_value=0)\n",
    "    label_ids = pad_sequence([ids.clone().detach() for ids in label_ids], batch_first=True, padding_value=-100)\n",
    "    return input_ids, label_ids  # Return original sentences as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "first_param_device = next(bert_model.parameters()).device\n",
    "print(first_param_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class BLSTMModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, output_dim, num_labels, dropout):\n",
    "#         super(BLSTMModel, self).__init__()\n",
    "#         self.blstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "#                              num_layers=2, bidirectional=True, batch_first=True)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "#         self.elu = nn.ELU()\n",
    "#         self.classifier = nn.Linear(output_dim, num_labels)\n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         blstm_out, _ = self.blstm(input_ids)\n",
    "#         blstm_out = self.dropout(blstm_out)\n",
    "#         linear_out = self.linear(blstm_out)\n",
    "#         elu_out = self.elu(linear_out)\n",
    "#         logits = self.classifier(elu_out)\n",
    "#         return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = BLSTMModel(embedding_dim, hidden_dim,\n",
    "                        output_dim, num_labels, dropout)\n",
    "# model.load_state_dict(torch.load('best_model7514.pt'))\n",
    "lstm_model.to(\"cuda:0\")\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=0.9, end_factor=0.1, total_iters=60)\n",
    "# optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "# Learning rate scheduling\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataset = NERDataset(\n",
    "    \"../../data/english/train-00000-of-00001.parquet\", tokenizer, bert_model)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = NERDataset(\n",
    "    \"../../data/english/validation-00000-of-00001.parquet\", tokenizer, bert_model)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_f1_score = 0\n",
    "patience = 15  # Number of epochs to wait for improvement\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(\"cuda:0\")\n",
    "        labels = labels.to(\"cuda:0\")\n",
    "        optimizer.zero_grad()\n",
    "        logits = lstm_model(input_ids)\n",
    "        loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation step\n",
    "    lstm_model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(\"cuda:0\")\n",
    "            labels = labels.to(\"cuda:0\")\n",
    "            logits = lstm_model(input_ids)\n",
    "            # predictions = torch.argmax(logits, dim=-1)\n",
    "            # for i, sentence in enumerate(input_ids):\n",
    "            #     for j, word_id in enumerate(sentence):\n",
    "            #         if word_id.item() == vocab['<PAD>']:\n",
    "            #             continue\n",
    "            #         # Get the predicted label name\n",
    "            #         predicted_label = reverse_label_map[predictions[i][j].item()]# Map the label ID back to its string representation\n",
    "            #         true_label = reverse_label_map[labels[i][j].item()]  # Map the label ID back to its string representation\n",
    "            #         true_labels.append(true_label)\n",
    "            #         pred_labels.append(predicted_label)\n",
    "            # loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # precision,recall,f1=evaluate(true_labels,pred_labels,False)\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\n",
    "            f\"Validation loss improved from {best_val_loss:.4f}--->{val_loss:.4f}\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(lstm_model.state_dict(), 'best_model3.pt')\n",
    "        epochs_without_improvement = 0\n",
    "    # if best_f1_score < f1:\n",
    "    #     print(f\"Validation f1 improved from {best_f1_score:.4f}--->{f1:.4f}\")\n",
    "    #     best_f1_score = f1\n",
    "    #     torch.save(lstm_model.state_dict(), 'best_model3.pt')\n",
    "    #     epochs_without_improvement = 0\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "    #     if epochs_without_improvement >= patience:\n",
    "    #         print(f'Early stopping at epoch {epoch + 1}')\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | criterion  | CrossEntropyLoss | 0     \n",
      "1 | blstm      | LSTM             | 11.6 M\n",
      "2 | dropout    | Dropout          | 0     \n",
      "3 | linear     | Linear           | 1.0 M \n",
      "4 | elu        | ELU              | 0     \n",
      "5 | classifier | Linear           | 9.2 K \n",
      "------------------------------------------------\n",
      "12.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.6 M    Total params\n",
      "50.438    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d1e770fac64750b0867c0354e8b4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49233ca2038641afbae06d058685ec5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeac09050cd741fbbaf6dea1b1ab89d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 251: 'val_seqeval_f1' reached 0.46188 (best 0.46188), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v7.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e2275ea37c4d6fa9c7577070828853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 502: 'val_seqeval_f1' reached 0.48440 (best 0.48440), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v8.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ab76a7006649e7ad7b821b21592508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 753: 'val_seqeval_f1' reached 0.48366 (best 0.48440), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v9.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e71143aadce435ab6e17d02142e7148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1004: 'val_seqeval_f1' reached 0.48572 (best 0.48572), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v7.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8833d12e44f3449893f8fdd33e716271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1255: 'val_seqeval_f1' reached 0.53028 (best 0.53028), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v9.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60cfceae2d249d493f24f04e2062ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1506: 'val_seqeval_f1' reached 0.49762 (best 0.53028), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v8.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d199640f4580427d8669d729e4b86fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1757: 'val_seqeval_f1' reached 0.51858 (best 0.53028), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v7.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72036d1a0cd14314aa1f5bea390f5298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2008: 'val_seqeval_f1' reached 0.53774 (best 0.53774), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v8.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650a22318fc84c368b0f15aaf748b311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 2259: 'val_seqeval_f1' reached 0.53086 (best 0.53774), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v7.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c629a4b7319241b1a5d77b6762617a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 2510: 'val_seqeval_f1' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cde096644484a39a25aa6bdea80c0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2761: 'val_seqeval_f1' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dad0dac2cc4d508371ac2c0349bd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 3012: 'val_seqeval_f1' reached 0.53351 (best 0.53774), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v9.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5165260884a41b0a7cdeda9275f8818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 3263: 'val_seqeval_f1' reached 0.53491 (best 0.53774), saving model to '/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpointtt-v7.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a982d4df7cf4d2f827e333e7cbf36d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from colleval import evaluate\n",
    "from datasets import load_metric\n",
    "wikineural_tags_list = ['O', 'B-PER', 'I-PER', 'B-ORG',\n",
    "                        'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "wikineural_tags_to_int = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3,\n",
    "                          'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "wikineural_int_to_tags = {v: k for k, v in wikineural_tags_to_int.items()}\n",
    "\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming you have a dataset called 'dataset'\n",
    "train_dataset = NERDataset(\n",
    "    \"../../data/merge/train.parquet\", tokenizer, bert_model)\n",
    "dataset_size = len(train_dataset)\n",
    "subset_size = int(0.1 * dataset_size)  # 10% of the dataset\n",
    "remaining_size = dataset_size - subset_size\n",
    "\n",
    "# Split the dataset into a subset and the remaining data\n",
    "train_dataset, _ = random_split(train_dataset, [subset_size, remaining_size])\n",
    "\n",
    "\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = NERDataset(\n",
    "    \"../../data/merge/dev.parquet\", tokenizer, bert_model)\n",
    "dataset_size = len(val_dataset)\n",
    "subset_size = int(0.1 * dataset_size)  # 10% of the dataset\n",
    "remaining_size = dataset_size - subset_size\n",
    "\n",
    "# Split the dataset into a subset and the remaining data\n",
    "val_dataset, _ = random_split(val_dataset, [subset_size, remaining_size])\n",
    "test_dataset = NERDataset(\n",
    "    \"../../data/merge/test.parquet\", tokenizer, bert_model)\n",
    "# Create a TensorBoard logger\n",
    "logger = TensorBoardLogger(\"logs/\", name=\"my_model_lstm\")\n",
    "seqeval_metric = load_metric(\"seqeval\")\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = bert_model.config.hidden_size\n",
    "hidden_dim = 512\n",
    "output_dim = 1024\n",
    "dropout = 0.33\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 60\n",
    "num_labels = 9\n",
    "class BLSTMModelLightning(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_labels, dropout):\n",
    "        super(BLSTMModelLightning, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                             num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        blstm_out, _ = self.blstm(input_ids)\n",
    "        blstm_out = self.dropout(blstm_out)\n",
    "        linear_out = self.linear(blstm_out)\n",
    "        elu_out = self.elu(linear_out)\n",
    "        logits = self.classifier(elu_out)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(\n",
    "            logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss, on_step=False,\n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(\n",
    "            logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        # Create a mask to exclude padding tokens\n",
    "        mask = labels.view(-1) != -100\n",
    "\n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        # acc = accuracy(preds, labels_flat, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i, label in enumerate(labels_flat):\n",
    "            true_labels.append(wikineural_int_to_tags[label.item()])\n",
    "            pred_labels.append(wikineural_int_to_tags[preds[i].item()])\n",
    "        results = seqeval_metric.compute(\n",
    "            predictions=[pred_labels], references=[true_labels])\n",
    "        self.log(\"val_seqeval_f1\", results['overall_f1'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(\n",
    "            logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=2).flatten()\n",
    "        # Create a mask to exclude padding tokens\n",
    "        mask = labels.view(-1) != -100\n",
    "\n",
    "        # Apply the mask to predictions and labels\n",
    "        preds = torch.argmax(logits, dim=2).view(-1)[mask]\n",
    "        labels_flat = labels.view(-1)[mask]\n",
    "        acc = accuracy(preds, labels_flat, task=\"multiclass\",\n",
    "                       num_classes=self.num_labels)\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i, label in enumerate(labels_flat):\n",
    "            true_labels.append(wikineural_int_to_tags[label.item()])\n",
    "            pred_labels.append(wikineural_int_to_tags[preds[i].item()])\n",
    "        results = seqeval_metric.compute(\n",
    "            predictions=[pred_labels], references=[true_labels])\n",
    "        self.log(\"test_loss\", loss, on_step=False,\n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_seqeval_f1\", results['overall_f1'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "class NERDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, test_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpointtt\",\n",
    "    save_top_k=3,\n",
    "    verbose=True,\n",
    "    monitor=\"val_seqeval_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "lstm_model = BLSTMModelLightning(\n",
    "    embedding_dim, hidden_dim, output_dim, num_labels, dropout)\n",
    "data_module = NERDataModule(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "trainer.fit(lstm_model, datamodule=data_module)\n",
    "# trainer.test(lstm_model,datamodule=data_module,ckpt_path=\"/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/best-checkpoint-v1.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
