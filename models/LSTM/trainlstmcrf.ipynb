{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/hjz/544/CSCI544-FinalProject/data/mBERT/fine and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30953/3484502526.py:272: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  seqeval_metric = load_metric(\"seqeval\")\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/lstm-8000v1-v2.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/lstm-8000v1-v2.ckpt\n",
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e939d81f9dd41258811678980ee6070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjz/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:519.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_loss            125.073974609375\n",
      "     test_seqeval_f1        0.5246785879135132\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, DeviceStatsMonitor\n",
    "from torchmetrics.functional import accuracy\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from datasets import load_metric\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast\n",
    "from torchcrf import CRF\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "# Constants\n",
    "MODEL_PATH = \"/home/hjz/544/CSCI544-FinalProject/data/mBERT/fine\"\n",
    "TRAIN_DATA_PATH = \"/home/hjz/544/CSCI544-FinalProject/data/merge/train.parquet\"\n",
    "VAL_DATA_PATH = \"/home/hjz/544/CSCI544-FinalProject/data/merge/dev.parquet\"\n",
    "TEST_DATA_PATH = \"/home/hjz/544/CSCI544-FinalProject/data/merge/test.parquet\"\n",
    "\n",
    "\n",
    "WIKINEURAL_TAGS_TO_INT = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3,\n",
    "                          'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8, 'PAD': 9}\n",
    "WIKINEURAL_INT_TO_TAGS = {v: k for k, v in WIKINEURAL_TAGS_TO_INT.items()}\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 768\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 1024\n",
    "DROPOUT = 0.33\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 60\n",
    "NUM_LABELS = 10\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "bert_model = BertModel.from_pretrained(MODEL_PATH)\n",
    "bert_model.eval()\n",
    "bert_model.to('cuda:0')\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "\n",
    "def pooling_embedding(tokenized_input, embeddings):\n",
    "    processed_embedding = []\n",
    "    current_embedding = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for i, word_idx in enumerate(tokenized_input):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "\n",
    "        if word_idx == previous_word_idx:\n",
    "            current_embedding.append(embeddings[i])\n",
    "        else:\n",
    "            if current_embedding:\n",
    "                processed_embedding.append(torch.mean(\n",
    "                    torch.stack(current_embedding), dim=0))\n",
    "                current_embedding.clear()\n",
    "            current_embedding.append(embeddings[i])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "    if current_embedding:\n",
    "        processed_embedding.append(torch.mean(\n",
    "            torch.stack(current_embedding), dim=0))\n",
    "\n",
    "    return torch.stack(processed_embedding)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, label_ids = zip(*batch)\n",
    "    input_ids = pad_sequence(\n",
    "        [ids.clone().detach() for ids in input_ids], batch_first=True, padding_value=0)\n",
    "    label_ids = pad_sequence(\n",
    "        [ids.clone().detach() for ids in label_ids], batch_first=True, padding_value=WIKINEURAL_TAGS_TO_INT['PAD'])\n",
    "    return input_ids, label_ids\n",
    "\n",
    "# Dataset\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_file_path, tokenizer, bert_model):\n",
    "        self.raw_dataset = pd.read_parquet(data_file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        current_row = self.raw_dataset.iloc[index]\n",
    "        sentence_words = current_row['tokens'].tolist()\n",
    "        encoded_words = tokenizer(sentence_words, return_tensors='pt',\n",
    "                                  is_split_into_words=True, truncation=True).to(\"cuda:0\")\n",
    "        embeddings = self.bert_model(**encoded_words)\n",
    "        pooled_embeddings = pooling_embedding(\n",
    "            encoded_words.word_ids(), embeddings.last_hidden_state[0])\n",
    "        labels = torch.tensor(current_row['ner_tags'].astype(int)).to(\"cuda:0\")\n",
    "        if pooled_embeddings.shape[0] < labels.shape[0]:\n",
    "            labels = labels[:pooled_embeddings.shape[0]]\n",
    "        assert pooled_embeddings.shape[0] == labels.shape[\n",
    "            0], f\"pooled_embeddings shape {pooled_embeddings.shape} and labels shape {labels.shape} are not equal, index {index}\"\n",
    "        return pooled_embeddings, labels\n",
    "\n",
    "# Data module\n",
    "\n",
    "\n",
    "class NERDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, test_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "all_true_labels=[]\n",
    "all_pred_labels=[]\n",
    "class BLSTMModelLightning(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_labels, dropout):\n",
    "        super(BLSTMModelLightning, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.transform_embeddings = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 2),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(embedding_dim * 2)\n",
    "        )\n",
    "        self.transform_embeddings2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                             num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_embeddings, labels=None):\n",
    "        transformed_embeddings = self.transform_embeddings(input_embeddings)\n",
    "        transformed_embeddings = self.transform_embeddings2(\n",
    "            transformed_embeddings)\n",
    "        blstm_out, _ = self.blstm(transformed_embeddings)\n",
    "        blstm_out = self.dropout(blstm_out)\n",
    "        emissions = self.classifier(blstm_out)\n",
    "        if labels is not None:\n",
    "            mask = (labels != WIKINEURAL_TAGS_TO_INT['PAD'])\n",
    "            loss = -self.crf(emissions, labels,mask=mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            decoded_sequence = self.crf.decode(emissions)\n",
    "            return decoded_sequence\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_embeddings, labels = batch\n",
    "        loss = self(input_embeddings, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=False,\n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_embeddings, labels = batch\n",
    "        loss = self(input_embeddings, labels)\n",
    "        decoded_labels = self(input_embeddings)\n",
    "\n",
    "        mask = labels != WIKINEURAL_TAGS_TO_INT['PAD']\n",
    "        true_labels = [\n",
    "            [WIKINEURAL_INT_TO_TAGS[label.item()]\n",
    "            for label, m in zip(label_seq, mask_seq) if m]\n",
    "            for label_seq, mask_seq in zip(labels, mask)\n",
    "        ]\n",
    "\n",
    "        pred_labels = [\n",
    "            [WIKINEURAL_INT_TO_TAGS[label]\n",
    "            for label, m in zip(label_seq, mask_seq) if m]\n",
    "            for label_seq, mask_seq in zip(decoded_labels, mask)\n",
    "        ]\n",
    "        all_true_labels.extend([label for sublist in true_labels for label in sublist])\n",
    "        all_pred_labels.extend([label for sublist in pred_labels for label in sublist])\n",
    "        results = seqeval_metric.compute(\n",
    "            predictions=pred_labels, references=true_labels)\n",
    "\n",
    "        self.log(\"val_seqeval_f1\", results['overall_f1'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_embeddings, labels = batch\n",
    "        loss = self(input_embeddings, labels)\n",
    "        decoded_labels = self(input_embeddings)\n",
    "\n",
    "        mask = labels != WIKINEURAL_TAGS_TO_INT['PAD']\n",
    "        true_labels = [\n",
    "            [WIKINEURAL_INT_TO_TAGS[label.item()]\n",
    "            for label, m in zip(label_seq, mask_seq) if m]\n",
    "            for label_seq, mask_seq in zip(labels, mask)\n",
    "        ]\n",
    "\n",
    "        pred_labels = [\n",
    "            [WIKINEURAL_INT_TO_TAGS[label]\n",
    "            for label, m in zip(label_seq, mask_seq) if m]\n",
    "            for label_seq, mask_seq in zip(decoded_labels, mask)\n",
    "        ]\n",
    "\n",
    "        results = seqeval_metric.compute(\n",
    "            predictions=pred_labels, references=true_labels)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_step=False,\n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_seqeval_f1\", results['overall_f1'],\n",
    "                 on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def configure_gradient_clipping(self, optimizer: optim.Optimizer, gradient_clip_val: int | float | None = None, gradient_clip_algorithm: str | None = None) -> None:\n",
    "        self.clip_gradients(\n",
    "            optimizer,\n",
    "            gradient_clip_val=gradient_clip_val,\n",
    "            gradient_clip_algorithm=gradient_clip_algorithm\n",
    "        )\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Load datasets\n",
    "    train_dataset = NERDataset(TRAIN_DATA_PATH, tokenizer, bert_model)\n",
    "    generator1 = torch.Generator().manual_seed(42)\n",
    "    # train_dataset, _ = random_split(train_dataset, [int(\n",
    "    #     0.1 * len(train_dataset)), len(train_dataset) - int(0.1 * len(train_dataset))],generator=generator1)\n",
    "    val_dataset = NERDataset(VAL_DATA_PATH, tokenizer, bert_model)\n",
    "    # val_dataset, _ = random_split(val_dataset, [int(\n",
    "    #     0.01 * len(val_dataset)), len(val_dataset) - int(0.01 * len(val_dataset))])\n",
    "    test_dataset = NERDataset(TEST_DATA_PATH, tokenizer, bert_model)\n",
    "    # test_dataset, _ = random_split(test_dataset, [int(\n",
    "    #     0.1 * len(test_dataset)), len(test_dataset) - int(0.1 * len(test_dataset))])\n",
    "    # Create data module\n",
    "    data_module = NERDataModule(\n",
    "        train_dataset, val_dataset, test_dataset, BATCH_SIZE)\n",
    "\n",
    "    # Create model\n",
    "    lstm_model = BLSTMModelLightning(\n",
    "        EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LABELS, DROPOUT)\n",
    "\n",
    "    # Set up logger and callbacks\n",
    "    logger = TensorBoardLogger(\"logs/\", name=\"my_model_lstm\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"lstm-8000v1\",\n",
    "        save_top_k=3,\n",
    "        verbose=True,\n",
    "        monitor=\"val_seqeval_f1\",\n",
    "        mode=\"max\"\n",
    "    )\n",
    "    device_stats = DeviceStatsMonitor()\n",
    "    seqeval_metric = load_metric(\"seqeval\")\n",
    "\n",
    "    # Set up trainer\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[checkpoint_callback, device_stats],\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_checkpointing=True,\n",
    "        enable_progress_bar=True,\n",
    "        logger=logger,\n",
    "        gradient_clip_val=1.0\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "   # trainer.fit(lstm_model, datamodule=data_module)\n",
    "\n",
    "    # Evaluate the model\n",
    "    trainer.test(lstm_model, datamodule=data_module, ckpt_path=\"/home/hjz/544/CSCI544-FinalProject/models/LSTM/checkpoints/lstm-8000v1-v2.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
