{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6ad615",
   "metadata": {},
   "source": [
    "# Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca76c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monkeydc\\.conda\\envs\\561\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pandas import read_parquet\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4546d4c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 尽量要用BertTokenizerFast, 避免会有莫名其妙的module问题\n",
    "raw_model_path = r\"C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\of\"\n",
    "# 储存mBERT的参数文件地址，我直接用的是绝对地址\n",
    "# tokenizer = AutoTokenizer.from_pretrained(raw_model_path,is_split_into_words=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(raw_model_path,is_split_into_words=True)\n",
    "# model = BertModel.from_pretrained(raw_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec378cff",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2129918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 储存数据的地址\n",
    "data_train = read_parquet(r\"C:\\Users\\monkeydc\\544\\PROJECT\\data\\merge\\raw.parquet\")\n",
    "data_test = read_parquet(r\"C:\\Users\\monkeydc\\544\\PROJECT\\data\\merge\\test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33065dd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>langs</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[#, #, ユ, リ, ウ, ス, ・, ベ, ー, リ, ッ, ク, #, 1, 9, ...</td>\n",
       "      <td>[0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, ...</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[PER: ユ リ ウ ス ・ ベ ー リ ッ ク]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[#, ル, ノ, ー, 、, 日, 産, 自, 動, 車, に, 資, 本, 参, 加, 。]</td>\n",
       "      <td>[0, 3, 4, 4, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[ORG: ル ノ ー, ORG: 日 産 自 動 車]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ソ, マ, リ, ラ, ン, ド, （, 事, 実, 上, 独, 立, し, た, 地, ...</td>\n",
       "      <td>[5, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[LOC: ソ マ リ ラ ン ド]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[R, E, D, I, R, E, C, T, #, ス, レ, イ, マ, ニ, エ, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[ORG: ス レ イ マ ニ エ ・ モ, ORG: ス ク]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[#, ', ', E, l, e, c, t, r, i, c, #, C, o, u, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[PER: S t e v e, PER: R e i c h]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [#, #, ユ, リ, ウ, ス, ・, ベ, ー, リ, ッ, ク, #, 1, 9, ...   \n",
       "1   [#, ル, ノ, ー, 、, 日, 産, 自, 動, 車, に, 資, 本, 参, 加, 。]   \n",
       "2  [ソ, マ, リ, ラ, ン, ド, （, 事, 実, 上, 独, 立, し, た, 地, ...   \n",
       "3  [R, E, D, I, R, E, C, T, #, ス, レ, イ, マ, ニ, エ, ...   \n",
       "4  [#, ', ', E, l, e, c, t, r, i, c, #, C, o, u, ...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0  [0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, ...   \n",
       "1   [0, 3, 4, 4, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0]   \n",
       "2  [5, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               langs  \\\n",
       "0  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "1  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "2  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "3  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "4  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "\n",
       "                              spans  \n",
       "0        [PER: ユ リ ウ ス ・ ベ ー リ ッ ク]  \n",
       "1      [ORG: ル ノ ー, ORG: 日 産 自 動 車]  \n",
       "2                [LOC: ソ マ リ ラ ン ド]  \n",
       "3  [ORG: ス レ イ マ ニ エ ・ モ, ORG: ス ク]  \n",
       "4  [PER: S t e v e, PER: R e i c h]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b7b84e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>langs</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[#, ヌ, ン, チ, ャ, ク, バ, ン, キ, ：, 吉, 水, 孝, 宏]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2]</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[PER: 吉 水 孝 宏]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[:, #, テ, レ, ビ, 東, 京, 系, ア, ニ, メ, 『, ジ, ュ, エ, ...</td>\n",
       "      <td>[0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 3, 4, 4, ...</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[ORG: テ レ ビ 東 京, ORG: ジ ュ エ ル ペ ッ ト, ORG: ハ ッ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[#, ゆ, う, ち, ょ, 銀, 行, A, T, M]</td>\n",
       "      <td>[0, 3, 4, 4, 4, 4, 4, 3, 4, 4]</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja]</td>\n",
       "      <td>[ORG: ゆ う ち ょ 銀 行, ORG: A T M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[', ', ', ノ, ー, ネ, ', ', ', (, ', ', ', N, o, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...</td>\n",
       "      <td>[LOC: イ タ リ ア 共 和 国, LOC: ピ エ モ ン テ 州, LOC: ト ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[#, ', ', ', バ, ー, リ, 県, ', ', ']</td>\n",
       "      <td>[0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0]</td>\n",
       "      <td>[ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja]</td>\n",
       "      <td>[LOC: バ ー リ 県]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0         [#, ヌ, ン, チ, ャ, ク, バ, ン, キ, ：, 吉, 水, 孝, 宏]   \n",
       "1  [:, #, テ, レ, ビ, 東, 京, 系, ア, ニ, メ, 『, ジ, ュ, エ, ...   \n",
       "2                     [#, ゆ, う, ち, ょ, 銀, 行, A, T, M]   \n",
       "3  [', ', ', ノ, ー, ネ, ', ', ', (, ', ', ', N, o, ...   \n",
       "4                  [#, ', ', ', バ, ー, リ, 県, ', ', ']   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2]   \n",
       "1  [0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 3, 4, 4, ...   \n",
       "2                     [0, 3, 4, 4, 4, 4, 4, 3, 4, 4]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4                  [0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0]   \n",
       "\n",
       "                                               langs  \\\n",
       "0  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "1  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "2           [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja]   \n",
       "3  [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, j...   \n",
       "4       [ja, ja, ja, ja, ja, ja, ja, ja, ja, ja, ja]   \n",
       "\n",
       "                                               spans  \n",
       "0                                     [PER: 吉 水 孝 宏]  \n",
       "1  [ORG: テ レ ビ 東 京, ORG: ジ ュ エ ル ペ ッ ト, ORG: ハ ッ ...  \n",
       "2                     [ORG: ゆ う ち ょ 銀 行, ORG: A T M]  \n",
       "3  [LOC: イ タ リ ア 共 和 国, LOC: ピ エ モ ン テ 州, LOC: ト ...  \n",
       "4                                     [LOC: バ ー リ 県]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca7eeb",
   "metadata": {},
   "source": [
    "# prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20bda0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得tag文件\n",
    "with open(r\"C:\\Users\\monkeydc\\544\\PROJECT\\data\\merge\\tags_2_idx.json\",\"r\") as file:\n",
    "    tags_2_idx = json.load(file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e92af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_tags = {tags_2_idx[tag]:tag for tag in tags_2_idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd2196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumped codes\n",
    "def merge_token_into_sentence(tokens:list):\n",
    "    sentence = ''\n",
    "    for token in tokens:\n",
    "        sentence += token + \" \"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c2b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = data_train['tokens'].values.tolist()\n",
    "tags_train = data_train['ner_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b724c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = data_test['tokens'].values.tolist()\n",
    "tags_test = data_test['ner_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cf6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sentences_train = sentences_train[0:2]\n",
    "# sample_tags_train = tags_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93e269b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sentences_test = sentences_test[0:2]\n",
    "# sample_tags_test = tags_test[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b9f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_label(tokenized_input, tags, tags_2_idx, idx_2_tags, label_all_tokens=True): \n",
    "        # tokenized_input refers to the sequences after tokenized\n",
    "        # tags refers to the original tags from dataset\n",
    "        # False:只为每个拆分token的第一个子词提供一个标签。\n",
    "        # True:在属于同一 token 的所有子词中提供相同的标签。\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        previous_word_idx = None\n",
    "        label_ids = []   \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)                \n",
    "            elif word_idx != previous_word_idx:\n",
    "                try:\n",
    "                    label_ids.append(tags_2_idx[idx_2_tags[tags[word_idx]]])\n",
    "                except:\n",
    "                    label_ids.append(-100) \n",
    "            else:\n",
    "                label_ids.append(tags_2_idx[idx_2_tags[tags[word_idx]]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx      \n",
    "        return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f76f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_label_by_case(tokenizer , sentence, tags, tags_2_idx, idx_2_tags, label_all_tokens=True): \n",
    "        tokenized_input = tokenizer(sentence)\n",
    "        # tokenized_input refers to the sequences after tokenized\n",
    "        # tags refers to the original tags from dataset\n",
    "        # False:只为每个拆分token的第一个子词提供一个标签。\n",
    "        # True:在属于同一 token 的所有子词中提供相同的标签。\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        previous_word_idx = None\n",
    "        label_ids = []   \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)                \n",
    "            elif word_idx != previous_word_idx:\n",
    "                try:\n",
    "                    label_ids.append(tags_2_idx[idx_2_tags[tags[word_idx]]])\n",
    "                except:\n",
    "                    label_ids.append(-100) \n",
    "            else:\n",
    "                label_ids.append(tags_2_idx[idx_2_tags[tags[word_idx]]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx      \n",
    "        return tokenized_input, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "788be7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample dataset\n",
    "# train_sentences = []\n",
    "# train_tags = []\n",
    "# for i in range(len(sample_sentences_train)):\n",
    "#     try:\n",
    "#         text_tokenized = tokenizer(sample_sentences_train[i].tolist(), padding='max_length',max_length=512, truncation=True,return_tensors=\"pt\", is_split_into_words=True)\n",
    "#         extend_tags = align_label(text_tokenized, sample_tags_train[i], tags_2_idx, idx_2_tags)\n",
    "#         train_sentences.append(text_tokenized)\n",
    "#         train_tags.append(extend_tags)\n",
    "#     except:\n",
    "#         print(merge_token_into_sentence(sentences_train[i]))\n",
    "# test_sentences = []\n",
    "# test_tags = []\n",
    "# for i in range(len(sample_sentences_test)):\n",
    "#     try:\n",
    "#         text_tokenized = tokenizer(sample_sentences_test[i].tolist(), padding='max_length',max_length=512, truncation=True,return_tensors=\"pt\", is_split_into_words=True)\n",
    "#         extend_tags = align_label(text_tokenized, sample_tags_test[i], tags_2_idx, idx_2_tags)\n",
    "#         test_sentences.append(text_tokenized)\n",
    "#         test_tags.append(extend_tags)\n",
    "#     except:\n",
    "#         print(merge_token_into_sentence(sentences_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82fdedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "train_tags = []\n",
    "for i in range(len(sentences_train)):\n",
    "    try:\n",
    "        text_tokenized = tokenizer(sentences_train[i].tolist(), padding='max_length',max_length=512, truncation=True,return_tensors=\"pt\", is_split_into_words=True)\n",
    "        extend_tags = align_label(text_tokenized, tags_train[i], tags_2_idx, idx_2_tags)\n",
    "        train_sentences.append(text_tokenized)\n",
    "        train_tags.append(extend_tags)\n",
    "    except:\n",
    "        print(merge_token_into_sentence(sentences_train[i]))\n",
    "test_sentences = []\n",
    "test_tags = []\n",
    "for i in range(len(sentences_test)):\n",
    "    try:\n",
    "        text_tokenized = tokenizer(sentences_test[i].tolist(), padding='max_length',max_length=512, truncation=True,return_tensors=\"pt\", is_split_into_words=True)\n",
    "        extend_tags = align_label(text_tokenized, tags_test[i], tags_2_idx, idx_2_tags)\n",
    "        test_sentences.append(text_tokenized)\n",
    "        test_tags.append(extend_tags)\n",
    "    except:\n",
    "        print(merge_token_into_sentence(sentences_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1ccc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_text_tokenized(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "    def get_labels(self, idx):\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_tokenized = self.get_text_tokenized(idx)\n",
    "        labels = self.get_labels(idx)\n",
    "        return text_tokenized, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae00c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataSequence(train_sentences, train_tags)\n",
    "test_dataset = DataSequence(test_sentences, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c17a0",
   "metadata": {},
   "source": [
    "# load official .bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7867ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "class BertModel_self(torch.nn.Module):\n",
    "    def __init__(self, raw_model_path, tags_2_idx):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(raw_model_path, num_labels=len(tags_2_idx))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask,\n",
    "                           labels=label, return_dict=False)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e687ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\of were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\of and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertModel_self(raw_model_path, tags_2_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591db79",
   "metadata": {},
   "source": [
    "# fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27975fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "def train_loop(model, train, val):\n",
    "    train_dataloader = DataLoader(train_dataset, num_workers=0, batch_size=1, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        model.train()\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_label = train_label[0].to(device)\n",
    "            mask = train_data['attention_mask'][0].to(device)\n",
    "            input_id = train_data['input_ids'][0].to(device)\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "            logits_clean = logits[0][train_label != -100]\n",
    "            label_clean = train_label[train_label != -100]\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        for val_data, val_label in test_dataloader:\n",
    "            val_label = val_label[0].to(device)\n",
    "            mask = val_data['attention_mask'][0].to(device)\n",
    "            input_id = val_data['input_ids'][0].to(device)\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "            logits_clean = logits[0][val_label != -100]\n",
    "            label_clean = val_label[val_label != -100]\n",
    "            predictions = logits_clean.argmax(dim=1)          \n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(val)\n",
    "        val_loss = total_loss_val / len(val)\n",
    "        if val_accuracy >= best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            bert_model = model.bert # only save bert part\n",
    "            torch.save(bert_model.state_dict(), r'C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\fine\\pytorch_model.bin')\n",
    "        print(\n",
    "            f'''Epochs: {epoch_num + 1} | \n",
    "                Loss: {total_loss_train / len(train): .3f} | \n",
    "                Accuracy: {total_acc_train / len(train): .3f} |\n",
    "                Val_Loss: {total_loss_val / len(val): .3f} | \n",
    "                Accuracy: {total_acc_val / len(val): .3f}''')\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5507b871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 120200/120200 [3:03:00<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | \n",
      "                Loss:  0.773 | \n",
      "                Accuracy:  0.732 |\n",
      "                Val_Loss:  0.638 | \n",
      "                Accuracy:  0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 120200/120200 [3:37:43<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | \n",
      "                Loss:  0.591 | \n",
      "                Accuracy:  0.802 |\n",
      "                Val_Loss:  0.581 | \n",
      "                Accuracy:  0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 120200/120200 [2:58:58<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | \n",
      "                Loss:  0.551 | \n",
      "                Accuracy:  0.816 |\n",
      "                Val_Loss:  0.518 | \n",
      "                Accuracy:  0.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 120200/120200 [2:59:57<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | \n",
      "                Loss:  0.516 | \n",
      "                Accuracy:  0.828 |\n",
      "                Val_Loss:  0.543 | \n",
      "                Accuracy:  0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 120200/120200 [3:18:14<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | \n",
      "                Loss:  0.876 | \n",
      "                Accuracy:  0.700 |\n",
      "                Val_Loss:  1.154 | \n",
      "                Accuracy:  0.632\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533890e",
   "metadata": {},
   "source": [
    "# model .bin save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40e676c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = model.bert # only save bert part\n",
    "# torch.save(bert_model.state_dict(), r'C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\fine\\pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6b452",
   "metadata": {},
   "source": [
    "# reload model sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3812df4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\fine were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\fine and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "fine_model_path = r'C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\fine'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(fine_model_path,is_split_into_words=True)\n",
    "model_reload = BertModel.from_pretrained(fine_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033003cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e95bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4eb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04216a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f791e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be88329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marks\n",
    "# [Errno 32] Broken pipe\n",
    "# 进程bug，由于进程的内存占有率是更具process走的，所以占有率远高于同等级的batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864f52e",
   "metadata": {},
   "source": [
    "# dumpcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3a5bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mBertModel(torch.nn.Module):\n",
    "    def __init__(self, raw_model_path):\n",
    "        super(mBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(raw_model_path)\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label)\n",
    "        return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel\n",
    "class mBertModel(BertPreTrainedModel):\n",
    "    def __init__(self, raw_model_path):\n",
    "        super(BertSoftmaxForNer, self).__init__(raw_model_path)\n",
    "        self.num_labels = 9\n",
    "        self.bert = BertModel(raw_model_path)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(768, 9)\n",
    "        self.loss_type = \"ce\"\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,labels=None):\n",
    "        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            assert self.loss_type in ['lsr', 'focal', 'ce']\n",
    "            if self.loss_type == 'lsr':\n",
    "                loss_fct = LabelSmoothingCrossEntropy(ignore_index=0)\n",
    "            elif self.loss_type == 'focal':\n",
    "                loss_fct = FocalLoss(ignore_index=0)\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(ignore_index=0)\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc1731e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\monkeydc\\544\\PROJECT\\data\\mBERT\\of were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = mBertModel(raw_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb073aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = DataSequence(train_sentences, train_tags)\n",
    "test_dataset = DataSequence(test_sentences, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e875a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21664355",
   "metadata": {},
   "source": [
    "# fine-tuning dumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18528dcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_pretrained_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8100\\358125187.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "optimizer = transformers.AdamW(model_parameters, lr=lr, correct_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd217d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py375",
   "language": "python",
   "name": "561"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
