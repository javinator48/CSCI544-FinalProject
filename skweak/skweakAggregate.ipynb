{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javin/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy, re\n",
    "import numpy as np\n",
    "import torch\n",
    "from skweak import heuristics, gazetteers, generative, utils\n",
    "from skweak import aggregation, utils\n",
    "from transformers import pipeline, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from skweak.base import SpanAnnotator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HuggingFaceAnnotator(SpanAnnotator):\n",
    "    def __init__(self, name, model, tokenizer, label2id, train_dataset, val_dataset, test_dataset):\n",
    "        super().__init__(name)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.id2label = {id: label for label, id in label2id.items()}\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.name = name\n",
    "        \n",
    "    def tokenize_and_align_labels(self, examples):\n",
    "        label_all_tokens = False\n",
    "        tokenized_inputs = self.tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "        labels = []\n",
    "        #for wikineural this would be called ner_tags but for wikiann it is tags\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                # ignored in the loss function.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                # We set the label for the first token of each word.\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                # the label_all_tokens flag.\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    def reverse_map_predictions(self, original_tokens, tokenized_inputs, predictions):\n",
    "        # Assuming predictions are aligned with the tokenized input (subtokens)\n",
    "        word_ids = tokenized_inputs.word_ids()  # Get word IDs for all tokens in the batch\n",
    "        reversed_predictions = []\n",
    "        current_word_id = None\n",
    "        current_word_predictions = []\n",
    "        \n",
    "        for word_id, prediction in zip(word_ids, predictions):\n",
    "            if word_id is None:\n",
    "                # Skipping special tokens like [CLS], [SEP], etc.\n",
    "                continue\n",
    "            \n",
    "            if word_id != current_word_id:\n",
    "                # Encountering a new word, decide the label for the previous word\n",
    "                if current_word_predictions:\n",
    "                    # You can implement different strategies here\n",
    "                    # For simplicity, taking the first prediction for the word\n",
    "                    reversed_predictions.append(current_word_predictions[0])\n",
    "                current_word_predictions = [prediction]\n",
    "                current_word_id = word_id\n",
    "            else:\n",
    "                # Accumulating predictions for subtokens of the same word\n",
    "                current_word_predictions.append(prediction)\n",
    "        \n",
    "        # Don't forget to add the prediction for the last word\n",
    "        if current_word_predictions:\n",
    "            reversed_predictions.append(current_word_predictions[0])\n",
    "\n",
    "        return [original_tokens, reversed_predictions]\n",
    "    \n",
    "    def compute_metrics(self, p):\n",
    "        metric = load_metric(\"seqeval\")\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [list(self.label2id.keys())[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [list(self.label2id.keys())[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "    \n",
    "    #Runs model on a single example\n",
    "    def predict_single_example(self, example):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        #tokenize input tokens\n",
    "        tokenized = self.tokenizer(example, truncation=True, is_split_into_words=True)\n",
    "        \n",
    "    \n",
    "        input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "        model = self.model.to(device)\n",
    "        #make prediction \n",
    "        \n",
    "        predictions = model(input_token)\n",
    "        model_logits = predictions.logits \n",
    "        model_predictions = torch.argmax(model_logits, dim=-1)\n",
    "        return model_predictions\n",
    "    \n",
    "    #Evaluates model and runs prediction on the whole test dataset \n",
    "    def predict_tag(self):\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
    "        #Tokenize the inputs to get the word ids. \n",
    "        print(\"tokenizing dataset\")\n",
    "        tokenized_traindataset = self.train_dataset.map(self.tokenize_and_align_labels, batched =True)\n",
    "        tokenized_testdataset = self.test_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        tokenized_evaldataset = self.val_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        #Prepare model for pytorch dataloader \n",
    "        #tokenized_dataset.set_format(type='torch', columns=['tokens', 'ner_tags', 'lang', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "        self.model.eval()\n",
    "        print(\"Defining a trainer object\")\n",
    "        args = TrainingArguments(\n",
    "            \"test\",\n",
    "            evaluation_strategy = \"steps\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            push_to_hub=True,\n",
    "            eval_steps=20000,\n",
    "            save_steps=20000,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            self.model,\n",
    "            args,\n",
    "            train_dataset =tokenized_traindataset, \n",
    "            eval_dataset = tokenized_evaldataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        print(\"Evaluate Using Model\")\n",
    "        print(trainer.evaluate())\n",
    "        print(\"Predict Using Model\")\n",
    "        predictions, labels, _ = trainer.predict(tokenized_testdataset)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_tags_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "wikineural_tags_to_int = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "wikineural_int_to_tags = {v: k for k,v in wikineural_tags_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"Babelscape/wikineural-multilingual-ner\"\n",
    "\n",
    "wikineural_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "wikineural_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"DunnBC22/roberta-base-finetuned-WikiNeural\"\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikineural_datasets = load_dataset(\"Babelscape/wikineural\")\n",
    "wikineural_train_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_val_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_test_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'lang'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 10747,\n",
       " 13751,\n",
       " 10379,\n",
       " 18286,\n",
       " 10105,\n",
       " 19561,\n",
       " 105868,\n",
       " 83302,\n",
       " 117,\n",
       " 11816,\n",
       " 10114,\n",
       " 10105,\n",
       " 11621,\n",
       " 10173,\n",
       " 11849,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] This division also contains the Ventana Wilderness, home to the California condor. [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.decode(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are able to get the word mapping of the tokenizer. This allows us to map the further tokenized words back to the original word tokens. Hence we use this to get the model's predictions in the token and tag dimensions of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ve',\n",
       " '##ntana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'con',\n",
       " '##dor',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True).word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  10747,  13751,  10379,  18286,  10105,  19561, 105868,  83302,\n",
       "            117,  11816,  10114,  10105,  11621,  10173,  11849,    119,    102]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wikineural_input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "wikineural_input_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_model= wikineural_model.to(device)\n",
    "model_prediction = wikineural_model(wikineural_input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_prediction.logits \n",
    "model_predictions = torch.argmax(model_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_map_predictions(original_tokens, tokenized_inputs, predictions):\n",
    "    # Assuming predictions are aligned with the tokenized input (subtokens)\n",
    "    word_ids = tokenized_inputs.word_ids()  # Get word IDs for all tokens in the batch\n",
    "    reversed_predictions = []\n",
    "    current_word_id = None\n",
    "    current_word_predictions = []\n",
    "    \n",
    "    for word_id, prediction in zip(word_ids, predictions):\n",
    "        if word_id is None:\n",
    "            # Skipping special tokens like [CLS], [SEP], etc.\n",
    "            continue\n",
    "        \n",
    "        if word_id != current_word_id:\n",
    "            # Encountering a new word, decide the label for the previous word\n",
    "            if current_word_predictions:\n",
    "                # You can implement different strategies here\n",
    "                # For simplicity, taking the first prediction for the word\n",
    "                reversed_predictions.append(current_word_predictions[0])\n",
    "            current_word_predictions = [prediction]\n",
    "            current_word_id = word_id\n",
    "        else:\n",
    "            # Accumulating predictions for subtokens of the same word\n",
    "            current_word_predictions.append(prediction)\n",
    "    \n",
    "    # Don't forget to add the prediction for the last word\n",
    "    if current_word_predictions:\n",
    "        reversed_predictions.append(current_word_predictions[0])\n",
    "\n",
    "    return [original_tokens, reversed_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'division',\n",
       "  'also',\n",
       "  'contains',\n",
       "  'the',\n",
       "  'Ventana',\n",
       "  'Wilderness',\n",
       "  ',',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'California',\n",
       "  'condor',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_map_predictions(wikineural_train_dataset[0][\"tokens\"], tokenized, model_predictions[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0]['ner_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the HuggingFaceAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_annotator = HuggingFaceAnnotator(\"wikineural_annotator\", wikineural_model, wikineural_tokenizer, wikineural_tags_to_int,wikineural_train_dataset,wikineural_val_dataset, wikineural_test_dataset  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_annotator = HuggingFaceAnnotator(\"roberta_annotator\", roberta_model, roberta_tokenizer, wikineural_tags_to_int, wikineural_train_dataset, wikineural_val_dataset, wikineural_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 6388.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:05<00:00, 64.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009427888318896294, 'eval_precision': 0.9722115753162132, 'eval_recall': 0.9755769230769231, 'eval_f1': 0.9738913419082359, 'eval_accuracy': 0.9967609909470374, 'eval_runtime': 6.2568, 'eval_samples_per_second': 479.476, 'eval_steps_per_second': 59.934}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:05<00:00, 68.02it/s] \n"
     ]
    }
   ],
   "source": [
    "wikineural_predictions = predictions = wikineural_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 14104.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:06<00:00, 60.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07395368814468384, 'eval_precision': 0.8651873951146746, 'eval_recall': 0.8923076923076924, 'eval_f1': 0.8785382940452523, 'eval_accuracy': 0.9829105003523608, 'eval_runtime': 6.255, 'eval_samples_per_second': 479.614, 'eval_steps_per_second': 59.952}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:06<00:00, 55.70it/s] \n"
     ]
    }
   ],
   "source": [
    "roberta_predictions = roberta_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from skweak import utils, aggregation\n",
    "\n",
    "# Assume `dataset` is your dataset, and `annotators` is a list of your HuggingFaceAnnotator instances\n",
    "nlp = spacy.blank(\"xx\")  # or load an existing SpaCy model\n",
    "\n",
    "# Function to apply HuggingFaceAnnotator predictions as skweak annotations\n",
    "def apply_annotations(tokens, annotator):\n",
    "    \n",
    "   \n",
    "    predictions = annotator.predict_single_example(tokens)\n",
    "    #map the predictions back to the original space\n",
    "    spaces = [True if token not in [',', '.'] else False for token in tokens[:-1]] + [False]\n",
    "    tokenized = wikineural_tokenizer(tokens, truncation=True, is_split_into_words=True)\n",
    "    mapped_to_original = annotator.reverse_map_predictions(tokens, tokenized,predictions[0].tolist())\n",
    "    #convert example which is a list of strings into a doc object so that our skweak aggregator understands\n",
    "    doc = spacy.tokens.Doc(nlp.vocab, words = tokens, spaces=spaces)\n",
    "   \n",
    "   \n",
    "    \n",
    "    # Initialize the span group for this annotator if it doesn't exist\n",
    "    if annotator.name not in doc.spans:\n",
    "        doc.spans[annotator.name] = []\n",
    "    for token, label_id in zip(doc, mapped_to_original[1]):\n",
    "        label = annotator.id2label[label_id]\n",
    "        # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "        span = Span(doc, token.i, token.i + 1, label=label)\n",
    "        doc.spans[annotator.name].append(span)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_model= wikineural_model.to(device)\n",
    "model_prediction = wikineural_model(wikineural_input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_prediction.logits \n",
    "model_predictions = torch.argmax(model_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs model on a single example\n",
    "def predict_single_example( example):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #tokenize input tokens\n",
    "    tokenized = wikineural_tokenizer(example, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "  \n",
    "    input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "    model = wikineural_model.to(device)\n",
    "    #make prediction \n",
    "    \n",
    "    predictions = model(input_token)\n",
    "    model_logits = predictions.logits \n",
    "    model_predictions = torch.argmax(model_logits, dim=-1)\n",
    "    return model_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single_example(['This',\n",
    " 'division',\n",
    " 'also',\n",
    " 'contains',\n",
    " 'the',\n",
    " 'Ventana',\n",
    " 'Wilderness',\n",
    " ',',\n",
    " 'home',\n",
    " 'to',\n",
    " 'the',\n",
    " 'California',\n",
    " 'condor',\n",
    " '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This, Label: O\n",
      "Text: division, Label: O\n",
      "Text: also, Label: O\n",
      "Text: contains, Label: O\n",
      "Text: the, Label: O\n",
      "Text: Ventana, Label: B-LOC\n",
      "Text: Wilderness, Label: I-LOC\n",
      "Text: ,, Label: O\n",
      "Text: home, Label: O\n",
      "Text: to, Label: O\n",
      "Text: the, Label: O\n",
      "Text: California, Label: B-LOC\n",
      "Text: condor, Label: O\n",
      "Text: ., Label: O\n"
     ]
    }
   ],
   "source": [
    "doc = apply_annotations(['This','division',\n",
    " 'also',\n",
    " 'contains',\n",
    " 'the',\n",
    " 'Ventana',\n",
    " 'Wilderness',\n",
    " ',',\n",
    " 'home',\n",
    " 'to',\n",
    " 'the',\n",
    " 'California',\n",
    " 'condor',\n",
    " '.'], wikineural_annotator)\n",
    "\n",
    "# Annotator's name - replace 'wikineural_annotator' with the actual name attribute of your annotator if different\n",
    "annotator_name = wikineural_annotator.name\n",
    "\n",
    "# Check if the annotator's name exists in doc.spans to avoid KeyError\n",
    "if annotator_name in doc.spans:\n",
    "    # Iterate through the spans in the specific annotator's span group\n",
    "    for span in doc.spans[annotator_name]:\n",
    "        # Print the text of each span and its label\n",
    "        print(f\"Text: {span.text}, Label: {span.label_}\")\n",
    "else:\n",
    "    print(f\"No annotations found for {annotator_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotators = [ roberta_annotator, wikineural_annotator]\n",
    "for annotator in annotators:\n",
    "    docs = [apply_annotations(sentence, annotator) for sentence in wikineural_train_dataset['tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following this tutorial https://github.com/NorskRegnesentral/skweak/wiki/Step-2:-Aggregation#basic-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Finished E-step with 3000 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1  -58038.11000689             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Finished E-step with 3000 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2  -36810.44755004  +21227.66245685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Finished E-step with 3000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3  -36810.44755004      +0.00000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, use skweak's aggregators to combine the annotations\n",
    "# For example, using the HMM aggregator\n",
    "aggregator = aggregation.HMM(\"aggregated\", out_labels = ['O', 'PER', 'ORG', 'LOC', 'MISC'])\n",
    "aggregator.add_label_group(\"PER\", [\"I-PER\", \"B-PER\"])\n",
    "aggregator.add_label_group(\"ORG\", [\"I-ORG\", \"B-ORG\"])\n",
    "aggregator.add_label_group(\"LOC\", [\"B-LOC\", \"I-LOC\"])\n",
    "aggregator.add_label_group(\"MISC\", [\"B-MISC\",\"I-MISC\"] )\n",
    "# aggregator = aggregation.HMM(\"aggregated\", out_labels = wikineural_tags_list)\n",
    "aggregated_docs = aggregator.fit(docs)\n",
    "\n",
    "# `aggregated_docs` now contains the aggregated labels for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This division also contains the Ventana Wilderness ,home to the California condor ."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" So here is the balance NBC has to consider : The Who ,' Animal Practice ' ."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "It is a protest song that \" creates a cinematic vista that tells of the singer 's search for a literal and physical America that seems to have disappeared ,along with the country 's beauty and ideals \" ."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    This\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    division\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    also\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    contains\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ventana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wilderness\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    home\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    to\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    condor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    &quot;\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    So\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    here\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    is\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    balance\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NBC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    has\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    to\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    consider\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    :\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Who\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    '\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Animal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Practice\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    '\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = list(aggregator.pipe(docs))\n",
    "utils.display_entities(docs[0], \"aggregated\")\n",
    "utils.display_entities(docs[1], \"aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_lists(doc_object): \n",
    "    # Initialize empty lists to store your sentence samples and their tags\n",
    "    sentence_samples = []\n",
    "    sentence_tags = []\n",
    "\n",
    "    for doc in doc_object:\n",
    "        # Initialize temporary lists for the current sentence sample\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        \n",
    "        # Iterate over each token in the Doc\n",
    "        for token in doc:\n",
    "            # Append the token text to the tokens list\n",
    "            tokens.append(token.text)\n",
    "            \n",
    "            # Check if there's an aggregated tag for this token\n",
    "            # Assuming the aggregated tags are stored in a SpanGroup named \"aggregated\"\n",
    "            if \"aggregated\" in doc.spans and doc.spans[\"aggregated\"]:\n",
    "                # Find any span that includes this token\n",
    "                span = next((span for span in doc.spans[\"aggregated\"] if span.start <= token.i < span.end), None)\n",
    "                if span:\n",
    "                    tags.append(span.label_)  # Use the label of the span\n",
    "                else:\n",
    "                    tags.append('O')  # Default to 'O' if no span includes this token\n",
    "            else:\n",
    "                # If there are no aggregated spans, default to 'O'\n",
    "                tags.append('O')\n",
    "        \n",
    "        # Append the tokens and tags for this sentence to your main lists\n",
    "        sentence_samples.append(tokens)\n",
    "        sentence_tags.append(tags)\n",
    "    return sentence_samples, sentence_tags\n",
    "\n",
    "# Now, `sentence_samples` and `sentence_tags` contain your data in the desired format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_samples, sentence_tags = convert_doc_to_lists(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory (GB): 9.77\n",
      "Current Memory Allocated (GB): 1.14\n",
      "Current Memory Reserved (GB): 1.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you want to check memory for the first GPU\n",
    "gpu_index = 0\n",
    "\n",
    "# Get total GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "# Convert bytes to GB for easier interpretation\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_allocated_gb = current_memory_allocated / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory reserved by PyTorch's memory allocator\n",
    "current_memory_reserved = torch.cuda.memory_reserved(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_reserved_gb = current_memory_reserved / (1024 ** 3)\n",
    "\n",
    "print(f\"Total GPU Memory (GB): {total_memory_gb:.2f}\")\n",
    "print(f\"Current Memory Allocated (GB): {current_memory_allocated_gb:.2f}\")\n",
    "print(f\"Current Memory Reserved (GB): {current_memory_reserved_gb:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
