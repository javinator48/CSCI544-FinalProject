{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javin/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy, re\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd \n",
    "from skweak import heuristics, gazetteers, generative, utils\n",
    "from skweak import aggregation, utils\n",
    "from transformers import pipeline, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from skweak.base import SpanAnnotator\n",
    "from pandas import read_parquet\n",
    "from transformers import AutoModelForTokenClassification,BertTokenizerFast, AutoTokenizer\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets,Dataset, DatasetDict\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HuggingFaceAnnotator(SpanAnnotator):\n",
    "    def __init__(self, name, model, tokenizer, label2id, train_dataset, val_dataset, test_dataset):\n",
    "        super().__init__(name)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.id2label = {id: label for label, id in label2id.items()}\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.name = name\n",
    "        \n",
    "    def tokenize_and_align_labels(self, examples):\n",
    "        label_all_tokens = False\n",
    "        tokenized_inputs = self.tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "        labels = []\n",
    "        #for wikineural this would be called ner_tags but for wikiann it is tags\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                # ignored in the loss function.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                # We set the label for the first token of each word.\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                # the label_all_tokens flag.\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    def reverse_map_predictions(self, original_tokens, tokenized_inputs, predictions):\n",
    "        # Assuming predictions are aligned with the tokenized input (subtokens)\n",
    "        word_ids = tokenized_inputs.word_ids()  # Get word IDs for all tokens in the batch\n",
    "        reversed_predictions = []\n",
    "        current_word_id = None\n",
    "        current_word_predictions = []\n",
    "        \n",
    "        for word_id, prediction in zip(word_ids, predictions):\n",
    "            if word_id is None:\n",
    "                # Skipping special tokens like [CLS], [SEP], etc.\n",
    "                continue\n",
    "            \n",
    "            if word_id != current_word_id:\n",
    "                # Encountering a new word, decide the label for the previous word\n",
    "                if current_word_predictions:\n",
    "                    # You can implement different strategies here\n",
    "                    # For simplicity, taking the first prediction for the word\n",
    "                    reversed_predictions.append(current_word_predictions[0])\n",
    "                current_word_predictions = [prediction]\n",
    "                current_word_id = word_id\n",
    "            else:\n",
    "                # Accumulating predictions for subtokens of the same word\n",
    "                current_word_predictions.append(prediction)\n",
    "        \n",
    "        # Don't forget to add the prediction for the last word\n",
    "        if current_word_predictions:\n",
    "            reversed_predictions.append(current_word_predictions[0])\n",
    "\n",
    "        return [original_tokens, reversed_predictions]\n",
    "    \n",
    "    def compute_metrics(self, p):\n",
    "        metric = load_metric(\"seqeval\")\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [list(self.label2id.keys())[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [list(self.label2id.keys())[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "    \n",
    "    #Runs model on a single example\n",
    "    def predict_single_example(self, example):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        #tokenize input tokens\n",
    "        tokenized = self.tokenizer(example, truncation=True, is_split_into_words=True)\n",
    "        \n",
    "    \n",
    "        input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "        model = self.model.to(device)\n",
    "        #make prediction \n",
    "        \n",
    "        predictions = model(input_token)\n",
    "        model_logits = predictions.logits \n",
    "        model_predictions = torch.argmax(model_logits, dim=-1)\n",
    "        return model_predictions\n",
    "    \n",
    "    #Evaluates model and runs prediction on the whole test dataset \n",
    "    def predict_tag(self):\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
    "        #Tokenize the inputs to get the word ids. \n",
    "        print(\"tokenizing dataset\")\n",
    "        tokenized_traindataset = self.train_dataset.map(self.tokenize_and_align_labels, batched =True)\n",
    "        tokenized_testdataset = self.test_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        tokenized_evaldataset = self.val_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        #Prepare model for pytorch dataloader \n",
    "        #tokenized_dataset.set_format(type='torch', columns=['tokens', 'ner_tags', 'lang', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "        self.model.eval()\n",
    "        print(\"Defining a trainer object\")\n",
    "        args = TrainingArguments(\n",
    "            \"test\",\n",
    "            evaluation_strategy = \"steps\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            push_to_hub=True,\n",
    "            eval_steps=20000,\n",
    "            save_steps=20000,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            self.model,\n",
    "            args,\n",
    "            train_dataset =tokenized_traindataset, \n",
    "            eval_dataset = tokenized_evaldataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        print(\"Evaluate Using Model\")\n",
    "        print(trainer.evaluate())\n",
    "        print(\"Predict Using Model\")\n",
    "        predictions, labels, metrics = trainer.predict(tokenized_testdataset)\n",
    "        print(\"Test Metrics: \", metrics)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        return predictions, labels, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_tags_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "wikineural_tags_to_int = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "wikineural_int_to_tags = {v: k for k,v in wikineural_tags_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"Babelscape/wikineural-multilingual-ner\"\n",
    "\n",
    "wikineural_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "wikineural_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"DunnBC22/roberta-base-finetuned-WikiNeural\"\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikineural_datasets = load_dataset(\"Babelscape/wikineural\")\n",
    "wikineural_train_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_val_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_test_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'lang'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 10747,\n",
       " 13751,\n",
       " 10379,\n",
       " 18286,\n",
       " 10105,\n",
       " 19561,\n",
       " 105868,\n",
       " 83302,\n",
       " 117,\n",
       " 11816,\n",
       " 10114,\n",
       " 10105,\n",
       " 11621,\n",
       " 10173,\n",
       " 11849,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] This division also contains the Ventana Wilderness, home to the California condor. [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.decode(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are able to get the word mapping of the tokenizer. This allows us to map the further tokenized words back to the original word tokens. Hence we use this to get the model's predictions in the token and tag dimensions of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ve',\n",
       " '##ntana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'con',\n",
       " '##dor',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True).word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  10747,  13751,  10379,  18286,  10105,  19561, 105868,  83302,\n",
       "            117,  11816,  10114,  10105,  11621,  10173,  11849,    119,    102]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wikineural_input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "wikineural_input_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_model= wikineural_model.to(device)\n",
    "model_prediction = wikineural_model(wikineural_input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_prediction.logits \n",
    "model_predictions = torch.argmax(model_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_map_predictions(original_tokens, tokenized_inputs, predictions):\n",
    "    # Assuming predictions are aligned with the tokenized input (subtokens)\n",
    "    word_ids = tokenized_inputs.word_ids()  # Get word IDs for all tokens in the batch\n",
    "    reversed_predictions = []\n",
    "    current_word_id = None\n",
    "    current_word_predictions = []\n",
    "    \n",
    "    for word_id, prediction in zip(word_ids, predictions):\n",
    "        if word_id is None:\n",
    "            # Skipping special tokens like [CLS], [SEP], etc.\n",
    "            continue\n",
    "        \n",
    "        if word_id != current_word_id:\n",
    "            # Encountering a new word, decide the label for the previous word\n",
    "            if current_word_predictions:\n",
    "                # You can implement different strategies here\n",
    "                # For simplicity, taking the first prediction for the word\n",
    "                reversed_predictions.append(current_word_predictions[0])\n",
    "            current_word_predictions = [prediction]\n",
    "            current_word_id = word_id\n",
    "        else:\n",
    "            # Accumulating predictions for subtokens of the same word\n",
    "            current_word_predictions.append(prediction)\n",
    "    \n",
    "    # Don't forget to add the prediction for the last word\n",
    "    if current_word_predictions:\n",
    "        reversed_predictions.append(current_word_predictions[0])\n",
    "\n",
    "    return [original_tokens, reversed_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'division',\n",
       "  'also',\n",
       "  'contains',\n",
       "  'the',\n",
       "  'Ventana',\n",
       "  'Wilderness',\n",
       "  ',',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'California',\n",
       "  'condor',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_map_predictions(wikineural_train_dataset[0][\"tokens\"], tokenized, model_predictions[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0]['ner_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the HuggingFaceAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_tags_list2 = ['O', 'BPER', 'IPER', 'BORG', 'IORG', 'BLOC', 'ILOC', 'BMISC', 'IMISC']\n",
    "wikineural_tags_to_int2 = {'O': 0, 'BPER': 1, 'IPER': 2, 'BORG': 3, 'IORG': 4, 'BLOC': 5, 'ILOC': 6, 'BMISC': 7, 'IMISC': 8}\n",
    "wikineural_int_to_tags2 = {v: k for k,v in wikineural_tags_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_annotator = HuggingFaceAnnotator(\"wikineural_annotator\", wikineural_model, wikineural_tokenizer, wikineural_tags_to_int2,wikineural_train_dataset,wikineural_val_dataset, wikineural_test_dataset  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_annotator = HuggingFaceAnnotator(\"roberta_annotator\", roberta_model, roberta_tokenizer, wikineural_tags_to_int2, wikineural_train_dataset, wikineural_val_dataset, wikineural_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 13933.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a trainer object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wikineural_predictions \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mwikineural_annotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 134\u001b[0m, in \u001b[0;36mHuggingFaceAnnotator.predict_tag\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining a trainer object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m     evaluation_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m,\n\u001b[1;32m    133\u001b[0m )\n\u001b[0;32m--> 134\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_traindataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenized_evaldataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluate Using Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mevaluate())\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/transformers/trainer.py:550\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_hf_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m    552\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/transformers/trainer.py:3830\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3827\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3828\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id\n\u001b[0;32m-> 3830\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3831\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;241m=\u001b[39m repo_url\u001b[38;5;241m.\u001b[39mrepo_id\n\u001b[1;32m   3832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/huggingface_hub/hf_api.py:3339\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3336\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_hf_headers(token\u001b[38;5;241m=\u001b[39mtoken, is_write_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3338\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 3339\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot create repo: another conflicting operation is in progress\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mtext:\n\u001b[1;32m   3341\u001b[0m         \u001b[38;5;66;03m# Since https://github.com/huggingface/moon-landing/pull/7272 (private repo), it is not possible to\u001b[39;00m\n\u001b[1;32m   3342\u001b[0m         \u001b[38;5;66;03m# concurrently create repos on the Hub for a same user. This is rarely an issue, except when running\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3345\u001b[0m         \u001b[38;5;66;03m# dependent libraries.\u001b[39;00m\n\u001b[1;32m   3346\u001b[0m         \u001b[38;5;66;03m# NOTE: If a fix is implemented server-side, we should be able to remove this retry mechanism.\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate repo failed due to a concurrency issue. Retrying...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/csci544finalproject/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wikineural_predictions = predictions = wikineural_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 14104.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:06<00:00, 60.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07395368814468384, 'eval_precision': 0.8651873951146746, 'eval_recall': 0.8923076923076924, 'eval_f1': 0.8785382940452523, 'eval_accuracy': 0.9829105003523608, 'eval_runtime': 6.255, 'eval_samples_per_second': 479.614, 'eval_steps_per_second': 59.952}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:06<00:00, 55.70it/s] \n"
     ]
    }
   ],
   "source": [
    "roberta_predictions = roberta_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from skweak import utils, aggregation\n",
    "\n",
    "# Assume `dataset` is your dataset, and `annotators` is a list of your HuggingFaceAnnotator instances\n",
    "nlp = spacy.blank(\"xx\")  # or load an existing SpaCy model\n",
    "\n",
    "# Function to apply HuggingFaceAnnotator predictions as skweak annotations\n",
    "def apply_annotations(tokens, annotator):\n",
    "    \n",
    "   \n",
    "    predictions = annotator.predict_single_example(tokens)\n",
    "    #map the predictions back to the original space\n",
    "    spaces = [True if token not in [',', '.'] else False for token in tokens[:-1]] + [False]\n",
    "    tokenized = wikineural_tokenizer(tokens, truncation=True, is_split_into_words=True)\n",
    "    mapped_to_original = annotator.reverse_map_predictions(tokens, tokenized,predictions[0].tolist())\n",
    "    #convert example which is a list of strings into a doc object so that our skweak aggregator understands\n",
    "    doc = spacy.tokens.Doc(nlp.vocab, words = tokens, spaces=spaces)\n",
    "   \n",
    "   \n",
    "    \n",
    "    # Initialize the span group for this annotator if it doesn't exist\n",
    "    if annotator.name not in doc.spans:\n",
    "        doc.spans[annotator.name] = []\n",
    "    for token, label_id in zip(doc, mapped_to_original[1]):\n",
    "        label = annotator.id2label[label_id]\n",
    "        # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "        span = Span(doc, token.i, token.i + 1, label=label)\n",
    "        doc.spans[annotator.name].append(span)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_model= wikineural_model.to(device)\n",
    "model_prediction = wikineural_model(wikineural_input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_prediction.logits \n",
    "model_predictions = torch.argmax(model_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs model on a single example\n",
    "def predict_single_example( example):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #tokenize input tokens\n",
    "    tokenized = wikineural_tokenizer(example, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "  \n",
    "    input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "    model = wikineural_model.to(device)\n",
    "    #make prediction \n",
    "    \n",
    "    predictions = model(input_token)\n",
    "    model_logits = predictions.logits \n",
    "    model_predictions = torch.argmax(model_logits, dim=-1)\n",
    "    return model_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single_example(['This',\n",
    " 'division',\n",
    " 'also',\n",
    " 'contains',\n",
    " 'the',\n",
    " 'Ventana',\n",
    " 'Wilderness',\n",
    " ',',\n",
    " 'home',\n",
    " 'to',\n",
    " 'the',\n",
    " 'California',\n",
    " 'condor',\n",
    " '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This, Label: O\n",
      "Text: division, Label: O\n",
      "Text: also, Label: O\n",
      "Text: contains, Label: O\n",
      "Text: the, Label: O\n",
      "Text: Ventana, Label: B-LOC\n",
      "Text: Wilderness, Label: I-LOC\n",
      "Text: ,, Label: O\n",
      "Text: home, Label: O\n",
      "Text: to, Label: O\n",
      "Text: the, Label: O\n",
      "Text: California, Label: B-LOC\n",
      "Text: condor, Label: O\n",
      "Text: ., Label: O\n"
     ]
    }
   ],
   "source": [
    "doc = apply_annotations(['This','division',\n",
    " 'also',\n",
    " 'contains',\n",
    " 'the',\n",
    " 'Ventana',\n",
    " 'Wilderness',\n",
    " ',',\n",
    " 'home',\n",
    " 'to',\n",
    " 'the',\n",
    " 'California',\n",
    " 'condor',\n",
    " '.'], wikineural_annotator)\n",
    "\n",
    "# Annotator's name - replace 'wikineural_annotator' with the actual name attribute of your annotator if different\n",
    "annotator_name = wikineural_annotator.name\n",
    "\n",
    "# Check if the annotator's name exists in doc.spans to avoid KeyError\n",
    "if annotator_name in doc.spans:\n",
    "    # Iterate through the spans in the specific annotator's span group\n",
    "    for span in doc.spans[annotator_name]:\n",
    "        # Print the text of each span and its label\n",
    "        print(f\"Text: {span.text}, Label: {span.label_}\")\n",
    "else:\n",
    "    print(f\"No annotations found for {annotator_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotators = [ roberta_annotator, wikineural_annotator]\n",
    "for annotator in annotators:\n",
    "    docs = [apply_annotations(sentence, annotator) for sentence in wikineural_train_dataset['tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following this tutorial https://github.com/NorskRegnesentral/skweak/wiki/Step-2:-Aggregation#basic-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_tags_list2 = ['O', 'BPER', 'IPER', 'BORG', 'IORG', 'BLOC', 'ILOC', 'BMISC', 'IMISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_map_to_int_non_prefixed = {'O': 0, 'BPER': 1, 'IPER': 2, 'BORG': 3, 'IORG': 4, 'BLOC': 5, 'ILOC': 6, 'BMISC': 7, 'IMISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Finished E-step with 3000 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1  -51447.50647039             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Finished E-step with 3000 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2  -30208.93105272  +21238.57541767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Finished E-step with 3000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3  -30208.93105272      -0.00000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, use skweak's aggregators to combine the annotations\n",
    "# For example, using the HMM aggregator\n",
    "aggregator = aggregation.HMM(\"aggregated\", out_labels = wikineural_tags_list2)\n",
    "# aggregator.add_label_group(\"PER\", [\"I-PER\", \"B-PER\"])\n",
    "# aggregator.add_label_group(\"ORG\", [\"I-ORG\", \"B-ORG\"])\n",
    "# aggregator.add_label_group(\"LOC\", [\"B-LOC\", \"I-LOC\"])\n",
    "# aggregator.add_label_group(\"MISC\", [\"B-MISC\",\"I-MISC\"] )\n",
    "# aggregator = aggregation.HMM(\"aggregated\", out_labels = wikineural_tags_list)\n",
    "aggregated_docs = aggregator.fit(docs)\n",
    "\n",
    "# `aggregated_docs` now contains the aggregated labels for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This division also contains the Ventana Wilderness ,home to the California condor ."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" So here is the balance NBC has to consider : The Who ,' Animal Practice ' ."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "It is a protest song that \" creates a cinematic vista that tells of the singer 's search for a literal and physical America that seems to have disappeared ,along with the country 's beauty and ideals \" ."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    This\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    division\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    also\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    contains\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ventana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BLOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wilderness\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ILOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    home\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    to\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BLOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    condor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    &quot;\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    So\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    here\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    is\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    balance\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NBC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    has\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    to\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    consider\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    :\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BMISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Who\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IMISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    '\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Animal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BMISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Practice\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IMISC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    '\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = list(aggregator.pipe(docs))\n",
    "utils.display_entities(docs[0], \"aggregated\")\n",
    "utils.display_entities(docs[1], \"aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_lists(doc_object): \n",
    "    # Initialize empty lists to store your sentence samples and their tags\n",
    "    sentence_samples = []\n",
    "    sentence_tags = []\n",
    "\n",
    "    for doc in doc_object:\n",
    "        # Initialize temporary lists for the current sentence sample\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        \n",
    "        # Iterate over each token in the Doc\n",
    "        for token in doc:\n",
    "            # Append the token text to the tokens list\n",
    "            tokens.append(token.text)\n",
    "            \n",
    "            # Check if there's an aggregated tag for this token\n",
    "            # Assuming the aggregated tags are stored in a SpanGroup named \"aggregated\"\n",
    "            if \"aggregated\" in doc.spans and doc.spans[\"aggregated\"]:\n",
    "                # Find any span that includes this token\n",
    "                span = next((span for span in doc.spans[\"aggregated\"] if span.start <= token.i < span.end), None)\n",
    "                if span:\n",
    "                    tags.append(span.label_)  # Use the label of the span\n",
    "                else:\n",
    "                    tags.append('O')  # Default to 'O' if no span includes this token\n",
    "            else:\n",
    "                # If there are no aggregated spans, default to 'O'\n",
    "                tags.append('O')\n",
    "        \n",
    "        # Append the tokens and tags for this sentence to your main lists\n",
    "        sentence_samples.append(tokens)\n",
    "        sentence_tags.append(tags)\n",
    "    return sentence_samples, sentence_tags\n",
    "\n",
    "# Now, `sentence_samples` and `sentence_tags` contain your data in the desired format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_samples, sentence_tags = convert_doc_to_lists(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tags_map_to_int_non_prefixed[tag] for tag in sentence_tags[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory (GB): 9.77\n",
      "Current Memory Allocated (GB): 1.14\n",
      "Current Memory Reserved (GB): 1.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you want to check memory for the first GPU\n",
    "gpu_index = 0\n",
    "\n",
    "# Get total GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "# Convert bytes to GB for easier interpretation\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_allocated_gb = current_memory_allocated / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory reserved by PyTorch's memory allocator\n",
    "current_memory_reserved = torch.cuda.memory_reserved(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_reserved_gb = current_memory_reserved / (1024 ** 3)\n",
    "\n",
    "print(f\"Total GPU Memory (GB): {total_memory_gb:.2f}\")\n",
    "print(f\"Current Memory Allocated (GB): {current_memory_allocated_gb:.2f}\")\n",
    "print(f\"Current Memory Reserved (GB): {current_memory_reserved_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Wikineural on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = read_parquet(r\"/home/javin/Coding/CSCI544/FinalProject/data/merge/train.parquet\")\n",
    "data_dev = read_parquet(r\"/home/javin/Coding/CSCI544/FinalProject/data/merge/dev.parquet\")\n",
    "data_test = read_parquet(r\"/home/javin/Coding/CSCI544/FinalProject/data/merge/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(data_train)\n",
    "validation_ds = Dataset.from_pandas(data_dev)\n",
    "test_ds = Dataset.from_pandas(data_test)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = train_ds\n",
    "ds['validation'] = validation_ds\n",
    "ds['test'] = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/home/javin/Coding/CSCI544/FinalProject/data/merge/tags_2_idx.json\",\"r\") as file:\n",
    "    tags_2_idx = json.load(file)\n",
    "file.close()\n",
    "idx_2_tags = {tags_2_idx[tag]:tag for tag in tags_2_idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Babelscape/wikineural-multilingual-ner\"\n",
    "\n",
    "wikineural_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "wikineural_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(idx_2_tags), label2id = tags_2_idx, id2label = idx_2_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_annotator = HuggingFaceAnnotator(\"wikineural_annotator\", wikineural_model, wikineural_tokenizer, tags_2_idx,train_ds,validation_ds, test_ds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120200/120200 [00:06<00:00, 19743.66 examples/s]\n",
      "Map: 100%|██████████| 40100/40100 [00:01<00:00, 21412.85 examples/s]\n",
      "Map: 100%|██████████| 40100/40100 [00:02<00:00, 19025.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5013/5013 [01:01<00:00, 70.79it/s] /home/javin/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 5013/5013 [01:13<00:00, 67.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7594166994094849, 'eval_precision': 0.5581081331039713, 'eval_recall': 0.581267217630854, 'eval_f1': 0.5694523081568715, 'eval_accuracy': 0.7615307808289491, 'eval_runtime': 73.9099, 'eval_samples_per_second': 542.552, 'eval_steps_per_second': 67.826}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5009/5013 [01:01<00:00, 82.50it/s] /home/javin/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 5013/5013 [01:14<00:00, 67.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:  {'test_loss': 1.7693740129470825, 'test_precision': 0.530257119061542, 'test_recall': 0.5575450429468428, 'test_f1': 0.5435588171183608, 'test_accuracy': 0.76395880283915, 'test_runtime': 74.2271, 'test_samples_per_second': 540.234, 'test_steps_per_second': 67.536}\n"
     ]
    }
   ],
   "source": [
    "wikineural_predictions, labels, metrics = wikineural_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate BiLSTM models. Since the Skweak Aggregation HMM requires doc objects, we need to convert the tokens in our prediction csvs into docs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import Span\n",
    "from skweak import utils, aggregation\n",
    "nlp = spacy.blank(\"xx\")\n",
    "def apply_annotations_from_csv(docs, base_filepath,model_names):\n",
    "    dataframe0 = pd.read_csv(base_filepath+ model_names[0]+'.csv')\n",
    "    dataframe1 = pd.read_csv(base_filepath+model_names[1]+'.csv')\n",
    "    dataframe2 = pd.read_csv(base_filepath+model_names[2]+'.csv')\n",
    "    dataframe3 = pd.read_csv(base_filepath+model_names[3]+'.csv')\n",
    "    prediction0 = dataframe0['predictions'].tolist()\n",
    "    prediction1 = dataframe1['predictions'].tolist()\n",
    "    prediction2 = dataframe2['predictions'].tolist()\n",
    "    prediction3 = dataframe3['predictions'].tolist()\n",
    "    new_docs = []\n",
    "    \n",
    "    \n",
    "    for idx, doc in tqdm(enumerate(docs), total=len(docs), desc=\"Processing Docs\"):\n",
    "        for model in model_names: \n",
    "            if model not in doc.spans:\n",
    "                doc.spans[model] = []\n",
    "        # Assuming the tokens are a string representation of a list       \n",
    "        for token, label in zip(doc, eval(prediction0[idx])):\n",
    "            \n",
    "            label = label.replace(\"-\", \"\")            \n",
    "            # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "            span = Span(doc, token.i, token.i + 1, label=label)\n",
    "            doc.spans[model_names[0]].append(span)\n",
    "        for token, label in zip(doc, eval(prediction1[idx])):\n",
    "            label = label.replace(\"-\", \"\")\n",
    "            # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "            span = Span(doc, token.i, token.i + 1, label=label)\n",
    "            doc.spans[model_names[1]].append(span)\n",
    "        for token, label in zip(doc, eval(prediction2[idx])): \n",
    "            label = label.replace(\"-\", \"\")\n",
    "            # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "            span = Span(doc, token.i, token.i + 1, label=label)\n",
    "            doc.spans[model_names[2]].append(span)\n",
    "        for token, label in zip(doc, eval(prediction3[idx])): \n",
    "            label = label.replace(\"-\", \"\")\n",
    "            # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "            span = Span(doc, token.i, token.i + 1, label=label)\n",
    "            doc.spans[model_names[3]].append(span)\n",
    "            \n",
    "            \n",
    "        new_docs.append(doc)\n",
    "       \n",
    "   \n",
    "    return new_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_docs(tokens_list): \n",
    "    docs = []\n",
    "    for sentence_tokens in tokens_list: \n",
    "        spaces = [True if token not in [',', '.'] else False for token in sentence_tokens[:-1]] + [False]\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words = sentence_tokens, spaces=spaces)\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tokens_to_docs(validation_ds['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Docs: 100%|██████████| 40100/40100 [00:05<00:00, 7213.89it/s] \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "base_filepath  = \"/home/javin/Coding/CSCI544/FinalProject/model_prediction_files/\" \n",
    "dev_models = [\"BiLSTM_CNN_dev\", \"BiLSTM_CNN_Attention_dev\", \"BiLSTM_CRF_dev\", \"BiLSTM_dev\"]\n",
    "annotated_docs = apply_annotations_from_csv(docs, base_filepath, dev_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_2_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Skweak hmm aggregation doesn't support prefix ner tags, we remove the - symbol from the tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_idx_2_tags = {k: v.replace(\"-\", \"\") for k, v in idx_2_tags.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_2_updated_idx = {v: k for k, v in updated_idx_2_tags.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'BPER': 1,\n",
       " 'IPER': 2,\n",
       " 'BORG': 3,\n",
       " 'IORG': 4,\n",
       " 'BLOC': 5,\n",
       " 'ILOC': 6,\n",
       " 'BMISC': 7,\n",
       " 'IMISC': 8}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_2_updated_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1 -694389.30140799             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2 -668828.43638017  +25560.86502782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3 -668433.37349517    +395.06288499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4 -668390.02034052     +43.35315465\n"
     ]
    }
   ],
   "source": [
    "from skweak import aggregation, utils\n",
    "\n",
    "# Assuming docs is a list of SpaCy Docs with entities from different models\n",
    "aggregator = aggregation.HMM(\"aggregated\", list(updated_idx_2_tags.values()))\n",
    "aggregated_docs = aggregator.fit_and_aggregate(annotated_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    佐\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">BLOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    賀\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ILOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    県\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ILOC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utils.display_entities(aggregated_docs[0], \"aggregated\")\n",
    "utils.display_entities(aggregated_docs[1], \"aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_samples, sentence_tags = convert_doc_to_lists(aggregated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['佐', '賀', '県']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLOC', 'ILOC', 'ILOC']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tags_2_updated_idx[tag] for tag in sentence_tags[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"tokens\":sentence_samples, \"predictions\": sentence_tags}\n",
    "        \n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"skweak_hmm_output.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tags_int = [[tags_2_updated_idx[tag] for tag in tag_list] for tag_list in sentence_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_tags = [[idx_2_tags[tag] for tag in tag_list] for tag_list in validation_ds['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels_tags = [[idx_2_tags[tag] for tag in tag_list] for tag_list in sentence_tags_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'I-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_labels_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'I-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = metric.compute(predictions=predictions_labels_tags, references=true_labels_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.8559121710353522,\n",
       "  'recall': 0.8453875687454602,\n",
       "  'f1': 0.850617316175511,\n",
       "  'number': 19274},\n",
       " 'ORG': {'precision': 0.7752102148863282,\n",
       "  'recall': 0.7738605981471118,\n",
       "  'f1': 0.7745348185948098,\n",
       "  'number': 16083},\n",
       " 'PER': {'precision': 0.8799659305226014,\n",
       "  'recall': 0.8736937481123528,\n",
       "  'f1': 0.8768186226964112,\n",
       "  'number': 16555},\n",
       " 'overall_precision': 0.8384404898212657,\n",
       " 'overall_recall': 0.8322545846817692,\n",
       " 'overall_f1': 0.8353360853046665,\n",
       " 'overall_accuracy': 0.9147208304725455}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Aggregation on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tokens_to_docs(test_ds['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Docs: 100%|██████████| 40100/40100 [00:05<00:00, 7061.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1 -717809.12528166             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2 -692187.86600776  +25621.25927390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3 -691745.69111780    +442.17488996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Finished E-step with 40100 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4 -691696.30944032     +49.38167748\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_filepath  = \"/home/javin/Coding/CSCI544/FinalProject/model_prediction_files/\" \n",
    "models = [\"BiLSTM_CNN_test\", \"BiLSTM_CNN_Attention_test\", \"BiLSTM_CRF_test\", \"BiLSTM_test\"]\n",
    "annotated_docs = apply_annotations_from_csv(docs, base_filepath, models)\n",
    "from skweak import aggregation, utils\n",
    "\n",
    "# Assuming docs is a list of SpaCy Docs with entities from different models\n",
    "aggregator = aggregation.HMM(\"aggregated\", list(updated_idx_2_tags.values()))\n",
    "aggregated_docs = aggregator.fit_and_aggregate(annotated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_samples, sentence_tags = convert_doc_to_lists(aggregated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.8421161183871301,\n",
       "  'recall': 0.8345426673479817,\n",
       "  'f1': 0.8383122882660917,\n",
       "  'number': 19570},\n",
       " 'ORG': {'precision': 0.7526363362458572,\n",
       "  'recall': 0.7389658028635664,\n",
       "  'f1': 0.7457384243365078,\n",
       "  'number': 16902},\n",
       " 'PER': {'precision': 0.8708760943932278,\n",
       "  'recall': 0.8730527784236224,\n",
       "  'f1': 0.8719630779948332,\n",
       "  'number': 17204},\n",
       " 'overall_precision': 0.823540461341949,\n",
       " 'overall_recall': 0.8167896266487816,\n",
       " 'overall_f1': 0.8201511523495959,\n",
       " 'overall_accuracy': 0.914015516687325}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tags_int = [[tags_2_updated_idx[tag] for tag in tag_list] for tag_list in sentence_tags]\n",
    "predictions_labels_tags = [[idx_2_tags[tag] for tag in tag_list] for tag_list in sentence_tags_int]\n",
    "true_labels_tags = [[idx_2_tags[tag] for tag in tag_list] for tag_list in test_ds['ner_tags']]\n",
    "results = metric.compute(predictions=predictions_labels_tags, references=true_labels_tags)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Aggregation on training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tokens_to_docs(train_ds['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Docs: 100%|██████████| 80100/80100 [00:12<00:00, 6512.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Number of processed documents: 41000\n",
      "Number of processed documents: 42000\n",
      "Number of processed documents: 43000\n",
      "Number of processed documents: 44000\n",
      "Number of processed documents: 45000\n",
      "Number of processed documents: 46000\n",
      "Number of processed documents: 47000\n",
      "Number of processed documents: 48000\n",
      "Number of processed documents: 49000\n",
      "Number of processed documents: 50000\n",
      "Number of processed documents: 51000\n",
      "Number of processed documents: 52000\n",
      "Number of processed documents: 53000\n",
      "Number of processed documents: 54000\n",
      "Number of processed documents: 55000\n",
      "Number of processed documents: 56000\n",
      "Number of processed documents: 57000\n",
      "Number of processed documents: 58000\n",
      "Number of processed documents: 59000\n",
      "Number of processed documents: 60000\n",
      "Number of processed documents: 61000\n",
      "Number of processed documents: 62000\n",
      "Number of processed documents: 63000\n",
      "Number of processed documents: 64000\n",
      "Number of processed documents: 65000\n",
      "Number of processed documents: 66000\n",
      "Number of processed documents: 67000\n",
      "Number of processed documents: 68000\n",
      "Number of processed documents: 69000\n",
      "Number of processed documents: 70000\n",
      "Number of processed documents: 71000\n",
      "Number of processed documents: 72000\n",
      "Number of processed documents: 73000\n",
      "Number of processed documents: 74000\n",
      "Number of processed documents: 75000\n",
      "Number of processed documents: 76000\n",
      "Number of processed documents: 77000\n",
      "Number of processed documents: 78000\n",
      "Number of processed documents: 79000\n",
      "Number of processed documents: 80000\n",
      "Finished E-step with 80100 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1 -1374591.24942578             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Number of processed documents: 41000\n",
      "Number of processed documents: 42000\n",
      "Number of processed documents: 43000\n",
      "Number of processed documents: 44000\n",
      "Number of processed documents: 45000\n",
      "Number of processed documents: 46000\n",
      "Number of processed documents: 47000\n",
      "Number of processed documents: 48000\n",
      "Number of processed documents: 49000\n",
      "Number of processed documents: 50000\n",
      "Number of processed documents: 51000\n",
      "Number of processed documents: 52000\n",
      "Number of processed documents: 53000\n",
      "Number of processed documents: 54000\n",
      "Number of processed documents: 55000\n",
      "Number of processed documents: 56000\n",
      "Number of processed documents: 57000\n",
      "Number of processed documents: 58000\n",
      "Number of processed documents: 59000\n",
      "Number of processed documents: 60000\n",
      "Number of processed documents: 61000\n",
      "Number of processed documents: 62000\n",
      "Number of processed documents: 63000\n",
      "Number of processed documents: 64000\n",
      "Number of processed documents: 65000\n",
      "Number of processed documents: 66000\n",
      "Number of processed documents: 67000\n",
      "Number of processed documents: 68000\n",
      "Number of processed documents: 69000\n",
      "Number of processed documents: 70000\n",
      "Number of processed documents: 71000\n",
      "Number of processed documents: 72000\n",
      "Number of processed documents: 73000\n",
      "Number of processed documents: 74000\n",
      "Number of processed documents: 75000\n",
      "Number of processed documents: 76000\n",
      "Number of processed documents: 77000\n",
      "Number of processed documents: 78000\n",
      "Number of processed documents: 79000\n",
      "Number of processed documents: 80000\n",
      "Finished E-step with 80100 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2 -1325699.35905616  +48891.89036962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Number of processed documents: 41000\n",
      "Number of processed documents: 42000\n",
      "Number of processed documents: 43000\n",
      "Number of processed documents: 44000\n",
      "Number of processed documents: 45000\n",
      "Number of processed documents: 46000\n",
      "Number of processed documents: 47000\n",
      "Number of processed documents: 48000\n",
      "Number of processed documents: 49000\n",
      "Number of processed documents: 50000\n",
      "Number of processed documents: 51000\n",
      "Number of processed documents: 52000\n",
      "Number of processed documents: 53000\n",
      "Number of processed documents: 54000\n",
      "Number of processed documents: 55000\n",
      "Number of processed documents: 56000\n",
      "Number of processed documents: 57000\n",
      "Number of processed documents: 58000\n",
      "Number of processed documents: 59000\n",
      "Number of processed documents: 60000\n",
      "Number of processed documents: 61000\n",
      "Number of processed documents: 62000\n",
      "Number of processed documents: 63000\n",
      "Number of processed documents: 64000\n",
      "Number of processed documents: 65000\n",
      "Number of processed documents: 66000\n",
      "Number of processed documents: 67000\n",
      "Number of processed documents: 68000\n",
      "Number of processed documents: 69000\n",
      "Number of processed documents: 70000\n",
      "Number of processed documents: 71000\n",
      "Number of processed documents: 72000\n",
      "Number of processed documents: 73000\n",
      "Number of processed documents: 74000\n",
      "Number of processed documents: 75000\n",
      "Number of processed documents: 76000\n",
      "Number of processed documents: 77000\n",
      "Number of processed documents: 78000\n",
      "Number of processed documents: 79000\n",
      "Number of processed documents: 80000\n",
      "Finished E-step with 80100 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3 -1324982.96140443    +716.39765173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Number of processed documents: 20000\n",
      "Number of processed documents: 21000\n",
      "Number of processed documents: 22000\n",
      "Number of processed documents: 23000\n",
      "Number of processed documents: 24000\n",
      "Number of processed documents: 25000\n",
      "Number of processed documents: 26000\n",
      "Number of processed documents: 27000\n",
      "Number of processed documents: 28000\n",
      "Number of processed documents: 29000\n",
      "Number of processed documents: 30000\n",
      "Number of processed documents: 31000\n",
      "Number of processed documents: 32000\n",
      "Number of processed documents: 33000\n",
      "Number of processed documents: 34000\n",
      "Number of processed documents: 35000\n",
      "Number of processed documents: 36000\n",
      "Number of processed documents: 37000\n",
      "Number of processed documents: 38000\n",
      "Number of processed documents: 39000\n",
      "Number of processed documents: 40000\n",
      "Number of processed documents: 41000\n",
      "Number of processed documents: 42000\n",
      "Number of processed documents: 43000\n",
      "Number of processed documents: 44000\n",
      "Number of processed documents: 45000\n",
      "Number of processed documents: 46000\n",
      "Number of processed documents: 47000\n",
      "Number of processed documents: 48000\n",
      "Number of processed documents: 49000\n",
      "Number of processed documents: 50000\n",
      "Number of processed documents: 51000\n",
      "Number of processed documents: 52000\n",
      "Number of processed documents: 53000\n",
      "Number of processed documents: 54000\n",
      "Number of processed documents: 55000\n",
      "Number of processed documents: 56000\n",
      "Number of processed documents: 57000\n",
      "Number of processed documents: 58000\n",
      "Number of processed documents: 59000\n",
      "Number of processed documents: 60000\n",
      "Number of processed documents: 61000\n",
      "Number of processed documents: 62000\n",
      "Number of processed documents: 63000\n",
      "Number of processed documents: 64000\n",
      "Number of processed documents: 65000\n",
      "Number of processed documents: 66000\n",
      "Number of processed documents: 67000\n",
      "Number of processed documents: 68000\n",
      "Number of processed documents: 69000\n",
      "Number of processed documents: 70000\n",
      "Number of processed documents: 71000\n",
      "Number of processed documents: 72000\n",
      "Number of processed documents: 73000\n",
      "Number of processed documents: 74000\n",
      "Number of processed documents: 75000\n",
      "Number of processed documents: 76000\n",
      "Number of processed documents: 77000\n",
      "Number of processed documents: 78000\n",
      "Number of processed documents: 79000\n",
      "Number of processed documents: 80000\n",
      "Finished E-step with 80100 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4 -1324907.00847859     +75.95292583\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "base_filepath  = \"/home/javin/Coding/CSCI544/FinalProject/model_prediction_files/\" \n",
    "models = [\"BiLSTM_CNN_train\", \"BiLSTM_CNN_Attention_train\", \"BiLSTM_CRF_train\", \"BiLSTM_train\"]\n",
    "annotated_docs = apply_annotations_from_csv(docs, base_filepath, models)\n",
    "from skweak import aggregation, utils\n",
    "\n",
    "# Assuming docs is a list of SpaCy Docs with entities from different models\n",
    "aggregator = aggregation.HMM(\"aggregated\", list(updated_idx_2_tags.values()))\n",
    "aggregated_docs = aggregator.fit_and_aggregate(annotated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_samples, sentence_tags = convert_doc_to_lists(aggregated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.8738757473355862,\n",
       "  'recall': 0.8735805420575319,\n",
       "  'f1': 0.8737281197614128,\n",
       "  'number': 38483},\n",
       " 'ORG': {'precision': 0.8092714041865061,\n",
       "  'recall': 0.7916569018045465,\n",
       "  'f1': 0.8003672496260865,\n",
       "  'number': 34136},\n",
       " 'PER': {'precision': 0.8983786580808372,\n",
       "  'recall': 0.8950850661625709,\n",
       "  'f1': 0.8967288378766142,\n",
       "  'number': 34914},\n",
       " 'overall_precision': 0.8616395840561093,\n",
       " 'overall_recall': 0.8545562757479099,\n",
       " 'overall_f1': 0.8580833123231644,\n",
       " 'overall_accuracy': 0.9319564201549997}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tags_int = [[tags_2_updated_idx[tag] for tag in tag_list] for tag_list in sentence_tags]\n",
    "predictions_labels_tags = [[idx_2_tags[tag] for tag in tag_list] for tag_list in sentence_tags_int]\n",
    "true_labels_tags = [[idx_2_tags[tag] for tag in tag_list] for tag_list in train_ds['ner_tags']]\n",
    "results = metric.compute(predictions=predictions_labels_tags, references=true_labels_tags)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
