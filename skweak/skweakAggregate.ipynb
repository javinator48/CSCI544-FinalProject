{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "import numpy as np\n",
    "from skweak import heuristics, gazetteers, generative, utils\n",
    "from skweak import aggregation, utils\n",
    "from transformers import pipeline, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from skweak.base import SpanAnnotator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HuggingFaceAnnotator(SpanAnnotator):\n",
    "    def __init__(self, name, model, tokenizer, label2id, train_dataset, val_dataset, test_dataset):\n",
    "        super().__init__(name)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.id2label = {id: label for label, id in label2id.items()}\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    def tokenize_and_align_labels(self, examples):\n",
    "        label_all_tokens = False\n",
    "        tokenized_inputs = self.tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "        labels = []\n",
    "        #for wikineural this would be called ner_tags but for wikiann it is tags\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                # ignored in the loss function.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                # We set the label for the first token of each word.\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                # the label_all_tokens flag.\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    def compute_metrics(self, p):\n",
    "        metric = load_metric(\"seqeval\")\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [list(self.label2id.keys())[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [list(self.label2id.keys())[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "    def predict_tag(self):\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
    "        #Tokenize the inputs to get the word ids. \n",
    "        print(\"tokenizing dataset\")\n",
    "        tokenized_traindataset = self.train_dataset.map(self.tokenize_and_align_labels, batched =True)\n",
    "        tokenized_testdataset = self.test_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        tokenized_evaldataset = self.val_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        #Prepare model for pytorch dataloader \n",
    "        #tokenized_dataset.set_format(type='torch', columns=['tokens', 'ner_tags', 'lang', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "        self.model.eval()\n",
    "        print(\"Defining a trainer object\")\n",
    "        args = TrainingArguments(\n",
    "            \"test\",\n",
    "            evaluation_strategy = \"steps\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            push_to_hub=True,\n",
    "            eval_steps=20000,\n",
    "            save_steps=20000,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            self.model,\n",
    "            args,\n",
    "            train_dataset =tokenized_traindataset, \n",
    "            eval_dataset = tokenized_evaldataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        print(\"Evaluate Using Model\")\n",
    "        print(trainer.evaluate())\n",
    "        print(\"Predict Using Model\")\n",
    "        predictions, labels, _ = trainer.predict(tokenized_testdataset)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_tags_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "wikineural_tags_to_int = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "wikineural_int_to_tags = {v: k for k,v in wikineural_tags_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"Babelscape/wikineural-multilingual-ner\"\n",
    "\n",
    "wikineural_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "wikineural_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"DunnBC22/roberta-base-finetuned-WikiNeural\"\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikineural_datasets = load_dataset(\"Babelscape/wikineural\")\n",
    "wikineural_train_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_val_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_test_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'lang'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 10747,\n",
       " 13751,\n",
       " 10379,\n",
       " 18286,\n",
       " 10105,\n",
       " 19561,\n",
       " 105868,\n",
       " 83302,\n",
       " 117,\n",
       " 11816,\n",
       " 10114,\n",
       " 10105,\n",
       " 11621,\n",
       " 10173,\n",
       " 11849,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] This division also contains the Ventana Wilderness, home to the California condor. [SEP]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.decode(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are able to get the word mapping of the tokenizer. This allows us to map the further tokenized words back to the original word tokens. Hence we use this to get the model's predictions in the token and tag dimensions of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ve',\n",
       " '##ntana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'con',\n",
       " '##dor',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, None]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True).word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_input_token = torch.tensor([tokenized['input_ids']]).to(\"cuda\")\n",
    "wikineural_input_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = wikineural_model(wikineural_input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_prediction.logits \n",
    "model_predictions = torch.argmax(model_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_map_predictions(original_tokens, tokenized_inputs, predictions):\n",
    "    # Assuming predictions are aligned with the tokenized input (subtokens)\n",
    "    word_ids = tokenized_inputs.word_ids()  # Get word IDs for all tokens in the batch\n",
    "    reversed_predictions = []\n",
    "    current_word_id = None\n",
    "    current_word_predictions = []\n",
    "    \n",
    "    for word_id, prediction in zip(word_ids, predictions):\n",
    "        if word_id is None:\n",
    "            # Skipping special tokens like [CLS], [SEP], etc.\n",
    "            continue\n",
    "        \n",
    "        if word_id != current_word_id:\n",
    "            # Encountering a new word, decide the label for the previous word\n",
    "            if current_word_predictions:\n",
    "                # You can implement different strategies here\n",
    "                # For simplicity, taking the first prediction for the word\n",
    "                reversed_predictions.append(current_word_predictions[0])\n",
    "            current_word_predictions = [prediction]\n",
    "            current_word_id = word_id\n",
    "        else:\n",
    "            # Accumulating predictions for subtokens of the same word\n",
    "            current_word_predictions.append(prediction)\n",
    "    \n",
    "    # Don't forget to add the prediction for the last word\n",
    "    if current_word_predictions:\n",
    "        reversed_predictions.append(current_word_predictions[0])\n",
    "\n",
    "    return [original_tokens, reversed_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'division',\n",
       "  'also',\n",
       "  'contains',\n",
       "  'the',\n",
       "  'Ventana',\n",
       "  'Wilderness',\n",
       "  ',',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'California',\n",
       "  'condor',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_map_predictions(wikineural_train_dataset[0][\"tokens\"], tokenized, model_predictions[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0]['ner_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the HuggingFaceAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_annotator = HuggingFaceAnnotator(\"wikineural_annotator\", wikineural_model, wikineural_tokenizer, wikineural_tags_to_int,wikineural_train_dataset,wikineural_val_dataset, wikineural_test_dataset  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_annotator = HuggingFaceAnnotator(\"roberta_annotator\", roberta_model, roberta_tokenizer, wikineural_tags_to_int, wikineural_train_dataset, wikineural_val_dataset, wikineural_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 12025.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:07<00:00, 51.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009427888318896294, 'eval_precision': 0.9722115753162132, 'eval_recall': 0.9755769230769231, 'eval_f1': 0.9738913419082359, 'eval_accuracy': 0.9967609909470374, 'eval_runtime': 7.3959, 'eval_samples_per_second': 405.628, 'eval_steps_per_second': 50.703}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:08<00:00, 45.41it/s]\n"
     ]
    }
   ],
   "source": [
    "wikineural_predictions = predictions = wikineural_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 12476.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:08<00:00, 43.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07395368814468384, 'eval_precision': 0.8651873951146746, 'eval_recall': 0.8923076923076924, 'eval_f1': 0.8785382940452523, 'eval_accuracy': 0.9829105003523608, 'eval_runtime': 8.5659, 'eval_samples_per_second': 350.224, 'eval_steps_per_second': 43.778}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:08<00:00, 45.64it/s]\n"
     ]
    }
   ],
   "source": [
    "roberta_predictions = roberta_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "nlp = spacy.blank(\"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikineural_train_dataset['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each list of tokens into a Doc object\n",
    "docs = [Doc(nlp.vocab, words=sample) for sample in wikineural_train_dataset['tokens']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This division also contains the Ventana Wilderness , home to the California condor . "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "division\n",
      "also\n",
      "contains\n",
      "the\n",
      "Ventana\n",
      "Wilderness\n",
      ",\n",
      "home\n",
      "to\n",
      "the\n",
      "California\n",
      "condor\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# for token in docs[0]: \n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from skweak import utils, aggregation\n",
    "\n",
    "# Assume `dataset` is your dataset, and `annotators` is a list of your HuggingFaceAnnotator instances\n",
    "nlp = spacy.blank(\"xx\")  # or load an existing SpaCy model\n",
    "\n",
    "# Function to apply HuggingFaceAnnotator predictions as skweak annotations\n",
    "def apply_annotations(doc, annotator):\n",
    "    # Assuming `doc` is a single SpaCy Doc object and `annotator` is an instance of HuggingFaceAnnotator\n",
    "    # You might need to adjust the input format according to your data preparation steps for HuggingFace models\n",
    "    predictions = annotator.predict_tag()\n",
    "    for token, label_id in zip(doc, predictions):\n",
    "        label = annotator.id2label[label_id]\n",
    "        # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "        doc.spans[annotator.name].append((token.idx, token.idx + len(token), label))\n",
    "    return doc\n",
    "\n",
    "# Convert dataset to SpaCy Docs and apply annotations\n",
    "docs = list(nlp.pipe(dataset))\n",
    "for annotator in annotators:\n",
    "    docs = [apply_annotations(doc, annotator) for doc in docs]\n",
    "\n",
    "# Now, use skweak's aggregators to combine the annotations\n",
    "# For example, using the HMM aggregator\n",
    "aggregator = aggregation.HMM(\"aggregated\", list_of_label_spaces=[annotator.name for annotator in annotators])\n",
    "aggregated_docs = aggregator.fit_transform(docs)\n",
    "\n",
    "# `aggregated_docs` now contains the aggregated labels for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak import aggregation, utils\n",
    "\n",
    "# Collect all your annotated Docs into a list\n",
    "docs = [annotate_doc(example) for example in tokenized_dataset]  # Assuming `annotate_doc` is your annotation function\n",
    "\n",
    "# Use an aggregator, e.g., majority voting or HMM, to combine annotations\n",
    "aggregator = aggregation.HMM(\"aggregated_labels\")\n",
    "aggregated_docs = aggregator.fit_transform(docs)\n",
    "\n",
    "# `aggregated_docs` now contains Docs with aggregated annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory (GB): 9.77\n",
      "Current Memory Allocated (GB): 1.14\n",
      "Current Memory Reserved (GB): 1.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you want to check memory for the first GPU\n",
    "gpu_index = 0\n",
    "\n",
    "# Get total GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "# Convert bytes to GB for easier interpretation\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_allocated_gb = current_memory_allocated / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory reserved by PyTorch's memory allocator\n",
    "current_memory_reserved = torch.cuda.memory_reserved(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_reserved_gb = current_memory_reserved / (1024 ** 3)\n",
    "\n",
    "print(f\"Total GPU Memory (GB): {total_memory_gb:.2f}\")\n",
    "print(f\"Current Memory Allocated (GB): {current_memory_allocated_gb:.2f}\")\n",
    "print(f\"Current Memory Reserved (GB): {current_memory_reserved_gb:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
