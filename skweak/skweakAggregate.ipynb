{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javin/anaconda3/envs/csci544finalproject/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy, re\n",
    "import numpy as np\n",
    "import torch\n",
    "from skweak import heuristics, gazetteers, generative, utils\n",
    "from skweak import aggregation, utils\n",
    "from transformers import pipeline, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from skweak.base import SpanAnnotator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HuggingFaceAnnotator(SpanAnnotator):\n",
    "    def __init__(self, name, model, tokenizer, label2id, train_dataset, val_dataset, test_dataset):\n",
    "        super().__init__(name)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.id2label = {id: label for label, id in label2id.items()}\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.name = name\n",
    "        \n",
    "    def tokenize_and_align_labels(self, examples):\n",
    "        label_all_tokens = False\n",
    "        tokenized_inputs = self.tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "        labels = []\n",
    "        #for wikineural this would be called ner_tags but for wikiann it is tags\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                # ignored in the loss function.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                # We set the label for the first token of each word.\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                # the label_all_tokens flag.\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    def reverse_map_predictions(self, original_tokens, tokenized_inputs, predictions):\n",
    "        # Assuming predictions are aligned with the tokenized input (subtokens)\n",
    "        word_ids = tokenized_inputs.word_ids()  # Get word IDs for all tokens in the batch\n",
    "        reversed_predictions = []\n",
    "        current_word_id = None\n",
    "        current_word_predictions = []\n",
    "        \n",
    "        for word_id, prediction in zip(word_ids, predictions):\n",
    "            if word_id is None:\n",
    "                # Skipping special tokens like [CLS], [SEP], etc.\n",
    "                continue\n",
    "            \n",
    "            if word_id != current_word_id:\n",
    "                # Encountering a new word, decide the label for the previous word\n",
    "                if current_word_predictions:\n",
    "                    # You can implement different strategies here\n",
    "                    # For simplicity, taking the first prediction for the word\n",
    "                    reversed_predictions.append(current_word_predictions[0])\n",
    "                current_word_predictions = [prediction]\n",
    "                current_word_id = word_id\n",
    "            else:\n",
    "                # Accumulating predictions for subtokens of the same word\n",
    "                current_word_predictions.append(prediction)\n",
    "        \n",
    "        # Don't forget to add the prediction for the last word\n",
    "        if current_word_predictions:\n",
    "            reversed_predictions.append(current_word_predictions[0])\n",
    "\n",
    "        return [original_tokens, reversed_predictions]\n",
    "    \n",
    "    def compute_metrics(self, p):\n",
    "        metric = load_metric(\"seqeval\")\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions = [\n",
    "            [list(self.label2id.keys())[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [list(self.label2id.keys())[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "    \n",
    "    #Runs model on a single example\n",
    "    def predict_single_example(self, example):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        #tokenize input tokens\n",
    "        tokenized = self.tokenizer(example, truncation=True, is_split_into_words=True)\n",
    "        \n",
    "    \n",
    "        input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "        model = self.model.to(device)\n",
    "        #make prediction \n",
    "        \n",
    "        predictions = model(input_token)\n",
    "        model_logits = predictions.logits \n",
    "        model_predictions = torch.argmax(model_logits, dim=-1)\n",
    "        return model_predictions\n",
    "    \n",
    "    #Evaluates model and runs prediction on the whole test dataset \n",
    "    def predict_tag(self):\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
    "        #Tokenize the inputs to get the word ids. \n",
    "        print(\"tokenizing dataset\")\n",
    "        tokenized_traindataset = self.train_dataset.map(self.tokenize_and_align_labels, batched =True)\n",
    "        tokenized_testdataset = self.test_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        tokenized_evaldataset = self.val_dataset.map(self.tokenize_and_align_labels, batched=True)\n",
    "        #Prepare model for pytorch dataloader \n",
    "        #tokenized_dataset.set_format(type='torch', columns=['tokens', 'ner_tags', 'lang', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "        self.model.eval()\n",
    "        print(\"Defining a trainer object\")\n",
    "        args = TrainingArguments(\n",
    "            \"test\",\n",
    "            evaluation_strategy = \"steps\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            push_to_hub=True,\n",
    "            eval_steps=20000,\n",
    "            save_steps=20000,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            self.model,\n",
    "            args,\n",
    "            train_dataset =tokenized_traindataset, \n",
    "            eval_dataset = tokenized_evaldataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        print(\"Evaluate Using Model\")\n",
    "        print(trainer.evaluate())\n",
    "        print(\"Predict Using Model\")\n",
    "        predictions, labels, _ = trainer.predict(tokenized_testdataset)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_tags_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "wikineural_tags_to_int = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "wikineural_int_to_tags = {v: k for k,v in wikineural_tags_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"Babelscape/wikineural-multilingual-ner\"\n",
    "\n",
    "wikineural_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "wikineural_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"DunnBC22/roberta-base-finetuned-WikiNeural\"\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=\"max_length\",truncation = True,  is_split_into_words=True)\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(wikineural_tags_list), label2id = wikineural_tags_to_int, id2label = wikineural_int_to_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikineural_datasets = load_dataset(\"Babelscape/wikineural\")\n",
    "wikineural_train_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_val_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])\n",
    "\n",
    "wikineural_test_dataset = concatenate_datasets([wikineural_datasets[\"train_en\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_es\"].select(range(1000)), \n",
    "                                      wikineural_datasets[\"train_fr\"].select(range(1000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'lang'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 10747,\n",
       " 13751,\n",
       " 10379,\n",
       " 18286,\n",
       " 10105,\n",
       " 19561,\n",
       " 105868,\n",
       " 83302,\n",
       " 117,\n",
       " 11816,\n",
       " 10114,\n",
       " 10105,\n",
       " 11621,\n",
       " 10173,\n",
       " 11849,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] This division also contains the Ventana Wilderness, home to the California condor. [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.decode(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are able to get the word mapping of the tokenizer. This allows us to map the further tokenized words back to the original word tokens. Hence we use this to get the model's predictions in the token and tag dimensions of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ve',\n",
       " '##ntana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'con',\n",
       " '##dor',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True).word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  10747,  13751,  10379,  18286,  10105,  19561, 105868,  83302,\n",
       "            117,  11816,  10114,  10105,  11621,  10173,  11849,    119,    102]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wikineural_input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "wikineural_input_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_model= wikineural_model.to(device)\n",
    "model_prediction = wikineural_model(wikineural_input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_prediction.logits \n",
    "model_predictions = torch.argmax(model_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_map_predictions(original_tokens, tokenized_inputs, predictions):\n",
    "    # Assuming predictions are aligned with the tokenized input (subtokens)\n",
    "    word_ids = tokenized_inputs.word_ids()  # Get word IDs for all tokens in the batch\n",
    "    reversed_predictions = []\n",
    "    current_word_id = None\n",
    "    current_word_predictions = []\n",
    "    \n",
    "    for word_id, prediction in zip(word_ids, predictions):\n",
    "        if word_id is None:\n",
    "            # Skipping special tokens like [CLS], [SEP], etc.\n",
    "            continue\n",
    "        \n",
    "        if word_id != current_word_id:\n",
    "            # Encountering a new word, decide the label for the previous word\n",
    "            if current_word_predictions:\n",
    "                # You can implement different strategies here\n",
    "                # For simplicity, taking the first prediction for the word\n",
    "                reversed_predictions.append(current_word_predictions[0])\n",
    "            current_word_predictions = [prediction]\n",
    "            current_word_id = word_id\n",
    "        else:\n",
    "            # Accumulating predictions for subtokens of the same word\n",
    "            current_word_predictions.append(prediction)\n",
    "    \n",
    "    # Don't forget to add the prediction for the last word\n",
    "    if current_word_predictions:\n",
    "        reversed_predictions.append(current_word_predictions[0])\n",
    "\n",
    "    return [original_tokens, reversed_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'division',\n",
       "  'also',\n",
       "  'contains',\n",
       "  'the',\n",
       "  'Ventana',\n",
       "  'Wilderness',\n",
       "  ',',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'California',\n",
       "  'condor',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_map_predictions(wikineural_train_dataset[0][\"tokens\"], tokenized, model_predictions[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0]['ner_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the HuggingFaceAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_annotator = HuggingFaceAnnotator(\"wikineural_annotator\", wikineural_model, wikineural_tokenizer, wikineural_tags_to_int,wikineural_train_dataset,wikineural_val_dataset, wikineural_test_dataset  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_annotator = HuggingFaceAnnotator(\"roberta_annotator\", roberta_model, roberta_tokenizer, wikineural_tags_to_int, wikineural_train_dataset, wikineural_val_dataset, wikineural_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n",
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:05<00:00, 66.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009427888318896294, 'eval_precision': 0.9722115753162132, 'eval_recall': 0.9755769230769231, 'eval_f1': 0.9738913419082359, 'eval_accuracy': 0.9967609909470374, 'eval_runtime': 5.6887, 'eval_samples_per_second': 527.359, 'eval_steps_per_second': 65.92}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:05<00:00, 68.03it/s] \n"
     ]
    }
   ],
   "source": [
    "wikineural_predictions = predictions = wikineural_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing dataset\n",
      "Defining a trainer object\n",
      "Evaluate Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:06<00:00, 61.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07395368814468384, 'eval_precision': 0.8651873951146746, 'eval_recall': 0.8923076923076924, 'eval_f1': 0.8785382940452523, 'eval_accuracy': 0.9829105003523608, 'eval_runtime': 6.127, 'eval_samples_per_second': 489.638, 'eval_steps_per_second': 61.205}\n",
      "Predict Using Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:06<00:00, 62.19it/s] \n"
     ]
    }
   ],
   "source": [
    "roberta_predictions = roberta_annotator.predict_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from skweak import utils, aggregation\n",
    "\n",
    "# Assume `dataset` is your dataset, and `annotators` is a list of your HuggingFaceAnnotator instances\n",
    "nlp = spacy.blank(\"xx\")  # or load an existing SpaCy model\n",
    "\n",
    "# Function to apply HuggingFaceAnnotator predictions as skweak annotations\n",
    "def apply_annotations(tokens, annotator):\n",
    "    \n",
    "   \n",
    "    predictions = annotator.predict_single_example(tokens)\n",
    "    #map the predictions back to the original space\n",
    "    spaces = [True if token not in [',', '.'] else False for token in tokens[:-1]] + [False]\n",
    "    tokenized = wikineural_tokenizer(tokens, truncation=True, is_split_into_words=True)\n",
    "    mapped_to_original = annotator.reverse_map_predictions(tokens, tokenized,predictions[0].tolist())\n",
    "    #convert example which is a list of strings into a doc object so that our skweak aggregator understands\n",
    "    doc = spacy.tokens.Doc(nlp.vocab, words = tokens, spaces=spaces)\n",
    "   \n",
    "   \n",
    "    \n",
    "    # Initialize the span group for this annotator if it doesn't exist\n",
    "    if annotator.name not in doc.spans:\n",
    "        doc.spans[annotator.name] = []\n",
    "    for token, label_id in zip(doc, mapped_to_original[1]):\n",
    "        label = annotator.id2label[label_id]\n",
    "        # Create a skweak annotation for each token. Adjust the span as needed.\n",
    "        span = Span(doc, token.i, token.i + 1, label=label)\n",
    "        doc.spans[annotator.name].append(span)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'division',\n",
       " 'also',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'Ventana',\n",
       " 'Wilderness',\n",
       " ',',\n",
       " 'home',\n",
       " 'to',\n",
       " 'the',\n",
       " 'California',\n",
       " 'condor',\n",
       " '.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikineural_train_dataset[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wikineural_tokenizer(wikineural_train_dataset[0][\"tokens\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikineural_model= wikineural_model.to(device)\n",
    "model_prediction = wikineural_model(wikineural_input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_prediction.logits \n",
    "model_predictions = torch.argmax(model_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs model on a single example\n",
    "def predict_single_example( example):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #tokenize input tokens\n",
    "    tokenized = wikineural_tokenizer(example, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "  \n",
    "    input_token = torch.tensor([tokenized['input_ids']]).to(device)\n",
    "    model = wikineural_model.to(device)\n",
    "    #make prediction \n",
    "    \n",
    "    predictions = model(input_token)\n",
    "    model_logits = predictions.logits \n",
    "    model_predictions = torch.argmax(model_logits, dim=-1)\n",
    "    return model_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single_example(['This',\n",
    " 'division',\n",
    " 'also',\n",
    " 'contains',\n",
    " 'the',\n",
    " 'Ventana',\n",
    " 'Wilderness',\n",
    " ',',\n",
    " 'home',\n",
    " 'to',\n",
    " 'the',\n",
    " 'California',\n",
    " 'condor',\n",
    " '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This, Label: O\n",
      "Text: division, Label: O\n",
      "Text: also, Label: O\n",
      "Text: contains, Label: O\n",
      "Text: the, Label: O\n",
      "Text: Ventana, Label: B-LOC\n",
      "Text: Wilderness, Label: I-LOC\n",
      "Text: ,, Label: O\n",
      "Text: home, Label: O\n",
      "Text: to, Label: O\n",
      "Text: the, Label: O\n",
      "Text: California, Label: B-LOC\n",
      "Text: condor, Label: O\n",
      "Text: ., Label: O\n"
     ]
    }
   ],
   "source": [
    "doc = apply_annotations(['This','division',\n",
    " 'also',\n",
    " 'contains',\n",
    " 'the',\n",
    " 'Ventana',\n",
    " 'Wilderness',\n",
    " ',',\n",
    " 'home',\n",
    " 'to',\n",
    " 'the',\n",
    " 'California',\n",
    " 'condor',\n",
    " '.'], wikineural_annotator)\n",
    "\n",
    "# Annotator's name - replace 'wikineural_annotator' with the actual name attribute of your annotator if different\n",
    "annotator_name = wikineural_annotator.name\n",
    "\n",
    "# Check if the annotator's name exists in doc.spans to avoid KeyError\n",
    "if annotator_name in doc.spans:\n",
    "    # Iterate through the spans in the specific annotator's span group\n",
    "    for span in doc.spans[annotator_name]:\n",
    "        # Print the text of each span and its label\n",
    "        print(f\"Text: {span.text}, Label: {span.label_}\")\n",
    "else:\n",
    "    print(f\"No annotations found for {annotator_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HMM() got an unexpected keyword argument 'list_of_label_spaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [apply_annotations(sentence, annotator) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m wikineural_train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Now, use skweak's aggregators to combine the annotations\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# For example, using the HMM aggregator\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m aggregator \u001b[38;5;241m=\u001b[39m \u001b[43maggregation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHMM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maggregated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_label_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mannotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mannotator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mannotators\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m aggregated_docs \u001b[38;5;241m=\u001b[39m aggregator\u001b[38;5;241m.\u001b[39mfit_transform(docs)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# `aggregated_docs` now contains the aggregated labels for each document\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: HMM() got an unexpected keyword argument 'list_of_label_spaces'"
     ]
    }
   ],
   "source": [
    "\n",
    "annotators = [ roberta_annotator, wikineural_annotator]\n",
    "for annotator in annotators:\n",
    "    docs = [apply_annotations(sentence, annotator) for sentence in wikineural_train_dataset['tokens']]\n",
    "\n",
    "# Now, use skweak's aggregators to combine the annotations\n",
    "# For example, using the HMM aggregator\n",
    "aggregator = aggregation.HMM(\"aggregated\", list_of_label_spaces=[annotator.name for annotator in annotators])\n",
    "aggregated_docs = aggregator.fit_transform(docs)\n",
    "\n",
    "# `aggregated_docs` now contains the aggregated labels for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory (GB): 9.77\n",
      "Current Memory Allocated (GB): 1.14\n",
      "Current Memory Reserved (GB): 1.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you want to check memory for the first GPU\n",
    "gpu_index = 0\n",
    "\n",
    "# Get total GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "# Convert bytes to GB for easier interpretation\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory allocated\n",
    "current_memory_allocated = torch.cuda.memory_allocated(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_allocated_gb = current_memory_allocated / (1024 ** 3)\n",
    "\n",
    "# Get current GPU memory reserved by PyTorch's memory allocator\n",
    "current_memory_reserved = torch.cuda.memory_reserved(gpu_index)\n",
    "# Convert bytes to GB\n",
    "current_memory_reserved_gb = current_memory_reserved / (1024 ** 3)\n",
    "\n",
    "print(f\"Total GPU Memory (GB): {total_memory_gb:.2f}\")\n",
    "print(f\"Current Memory Allocated (GB): {current_memory_allocated_gb:.2f}\")\n",
    "print(f\"Current Memory Reserved (GB): {current_memory_reserved_gb:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci544finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
